# -*- coding: utf-8 -*-
"""GPU Version of iml task 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8GUk9nxZeSmUVKZwHrqoeTdlF_j8tFw

# Neural Networks to predict protein activation

### Improvement ideas
* Look at one hot key encoding, are we dropping one of the 21 features? do we need to do so?
* One vector with four 1s, we might be loosing information
* Class weights
* Number of epochs
* batch size
* Neural Network (number of layers, where to put drop out layer, activations)
* Optimizer, loss function for f1

#### Set up the directories, load libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import pickle
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow.python.util import deprecation
deprecation._PRINT_DEPRECATION_WARNINGS = False

import keras
import keras.backend as K
from keras import Sequential
from keras.layers import Dense, Dropout
from keras.layers import BatchNormalization
from keras import regularizers
from keras.optimizers import SGD
from keras.callbacks import Callback,ModelCheckpoint

from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.utils import class_weight
from sklearn.preprocessing import LabelEncoder

tf.test.gpu_device_name()
from keras import backend as K
import keras.backend.tensorflow_backend as tfback
def _get_available_gpus():  

    if tfback._LOCAL_DEVICES is None:  
        devices = tf.config.list_logical_devices()  
        tfback._LOCAL_DEVICES = [x.name for x in devices]  
    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]


tfback._get_available_gpus = _get_available_gpus
tfback._get_available_gpus()

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

#!if [ ! -f Archive.zip ]; then wget -nv https://drive.google.com/open?id=1g7aT8cMkFAFlk6wxkiEH3mgFVp2Xa1l9 -O Archive.zip; fi

# from google.colab import drive
# drive.mount('/content/drive', force_remount= True)
# import os
# os.chdir("/content/drive/My Drive/IML/IML_Projects/task_3")
# os.getcwd()

import os
os.getcwd()

! ls

"""#### Load data & data inspection"""

dat_train = pd.read_csv("./Data/train.csv")
dat_test = pd.read_csv("./Data/test.csv")

# check class balance on activation
dat_train['Active'].value_counts()

"""#### Pre-process data"""

import re

def split_convert(word_inp): 
    return [ord(i) for i in word_inp]

train_seqs = [split_convert(i) for i in dat_train.iloc[:,0]]
train_labels = [i for i in dat_train.iloc[:,1]]
test_seqs = [split_convert(i) for i in dat_test.iloc[:,0]]

# binary encode
onehot_encoder = OneHotEncoder(sparse=False)
train_seqs_onehot = onehot_encoder.fit_transform(train_seqs)
test_seqs_onehot = onehot_encoder.transform(test_seqs)

"""#### Define Neural Network Architecture and Model"""

# functions to determine metrics f1, precision and recall
# taken from: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model

def get_recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def get_precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def get_f1(y_true, y_pred):
    precision = get_precision(y_true, y_pred)
    recall = get_recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# determine class imbalance
class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels), train_labels)
class_weight_dict = dict(enumerate(class_weights))

# class_weight = {0:1, 1:12}
NEPOCHS = 120
BATCHSIZE = 64
VALIDATIONSPLIT = 0.2
HIDDENSIZE = 80

# opt = SGD(lr=0.01, momentum=0.9)
opt = 'adam'

def create_model():
  model = Sequential()
  model.add(Dense(HIDDENSIZE, input_dim = 80, activation='relu', kernel_initializer= 'lecun_normal'))
  model.add(Dense(110, input_dim = HIDDENSIZE, activation='relu', kernel_initializer= 'lecun_normal'))
  model.add(Dropout(0.5))
  model.add(BatchNormalization())
  model.add(Dense(1, input_dim = 110, activation='sigmoid'))

  model.compile(optimizer=opt,
                loss='binary_crossentropy',
 #               loss='mean_squared_error',
                metrics=['accuracy', get_f1])

  return model

"""#### Model Selection / training"""

kfold_splits = 10
folds = list(StratifiedKFold(n_splits=kfold_splits, shuffle=True, random_state=1).split(train_seqs, train_labels))

device_name = tf.test.gpu_device_name()
device_name

from keras import backend as K
K.tensorflow_backend._get_available_gpus()

# Convert labels to categorical one-hot encoding
#train_labels_onehot = keras.utils.to_categorical(train_labels, num_classes=2)

mode_path = './models/mlp_v2.h5'

model = None
model = create_model()
model.summary()

best_fold = -1
best_score = 0
best_model = None

for index, (train_indices, val_indices) in enumerate(folds):
  print("Training on fold " + str(index+1) + "/10...")
  # Generate batches from indices
  xtrain, xval = train_seqs_onehot[train_indices], train_seqs_onehot[val_indices]
  #ytrain, yval = train_labels_onehot[train_indices], train_labels_onehot[val_indices]
  ytrain = np.array(train_labels)[train_indices.astype(int)]
  yval = np.array(train_labels)[val_indices.astype(int)]

  # xtrain_onehot = onehot_encoder.transform(xtrain)
  # xval_onehot = onehot_encoder.transform(xval)
  # ytrain_onehot = keras.utils.to_categorical(y_train, num_classes=2)
  # yval_onehot = keras.utils.to_categorical(y_val, num_classes=2)

  model = None
  model = create_model()

  # class wight for the train set
  class_weights = class_weight.compute_class_weight('balanced', np.unique(ytrain), ytrain)
  class_weight_dict = dict(enumerate(class_weights))

  # model.summary()
  with tf.device('/device:GPU:0'):
    callbacks = [ModelCheckpoint(filepath=mode_path, save_best_only=True)]
    model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = NEPOCHS, batch_size=BATCHSIZE, verbose = 0 ,
              callbacks=callbacks, class_weight = class_weight_dict)  # starts training

  # get the best fold based on the best f1 score
  y_pred = model.predict_classes(xval, batch_size=BATCHSIZE, verbose=1)
  y_train = model.predict_classes(xtrain, batch_size=BATCHSIZE, verbose=1)
  # y_pred_bool = np.argmax(y_pred, axis=1)
  y_pred_bool = y_pred.astype(int)
  #tmp_score = metrics.f1_score(np.argmax(yval, axis=1),y_pred)
  tmp_score = metrics.f1_score(yval,y_pred)
  score_train = metrics.f1_score(ytrain,y_train)
  print("F1 score for this fold is : ", tmp_score, score_train)
  if(tmp_score > best_score):
    best_fold = index
    best_model = model

# train model on entire data set
# class wight for the train set
class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels), train_labels)
class_weight_dict = dict(enumerate(class_weights))

NEPOCHS = 120
# model.summary()
callbacks = [ModelCheckpoint(filepath=mode_path, save_best_only=True)]
with tf.device('/device:GPU:0'):
  model.fit(train_seqs_onehot,train_labels, validation_split=0, epochs = NEPOCHS, batch_size=BATCHSIZE, verbose = 0 ,
            callbacks=callbacks, class_weight = class_weight_dict)  # starts training

# Training Error
y_pred = best_model.predict_classes(train_seqs_onehot, batch_size=BATCHSIZE, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(train_labels, y_pred))

"""#### Prediction on test data"""

y_pred = model.predict_classes(test_seqs_onehot, batch_size=BATCHSIZE,verbose = 1)
# res = np.argmax(y_pred, axis=1)
print(np.sum(y_pred))

# res = pd.DataFrame(res)
res = pd.DataFrame(y_pred)

"""#### Save results"""

res.to_csv("./prediction.csv", index=False, header=False)

