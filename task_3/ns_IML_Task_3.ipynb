{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ns_IML_Task_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PZ4JK9FA1my-",
        "drNBDblM15cR",
        "wTE0Z3et2N82",
        "V4wULYMI8woU",
        "OttnRkXZ8zil"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmadrezauf/IML_Projects/blob/master/task_3/ns_IML_Task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ZvOZjQhVaf",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks to predict protein activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ4JK9FA1my-",
        "colab_type": "text"
      },
      "source": [
        "#### Set up the directories, load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLodKHnmhaFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7shaT-b1x4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!if [ ! -f Archive.zip ]; then wget -nv https://drive.google.com/open?id=1g7aT8cMkFAFlk6wxkiEH3mgFVp2Xa1l9 -O Archive.zip; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBkOnoh0pLVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f67d3a82-4ba1-4d6b-cd01-c1f9c460c728"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/IML/IML_Projects/task_3\")\n",
        "os.getcwd()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IML/IML_Projects/task_3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4i-_fs24jYr",
        "colab_type": "code",
        "outputId": "43fa5ae4-e189-48ae-f495-1cce081bdcb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/IML/IML_Projects/task_3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLfAjJV7ouEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f6d6c5cd-2f49-4438-fb31-be8aeeb13775"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Data\t\t models\t\t      'on the server'\t     prediction.csv\n",
            " iml_task_3.py\t ns_IML_Task_3.ipynb  'prediction (1).csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drNBDblM15cR",
        "colab_type": "text"
      },
      "source": [
        "#### Load data & data inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQXuKcg4h71k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dat_train = pd.read_csv(\"./Data/train.csv\")\n",
        "dat_test = pd.read_csv(\"./Data/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgASSA0s-eT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "66185768-7cf8-49e7-8b86-3b7eb0991c86"
      },
      "source": [
        "# check class balance on activation\n",
        "dat_train['Active'].value_counts()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    107787\n",
              "1      4213\n",
              "Name: Active, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTE0Z3et2N82",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QH_sN8RiI_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def split_convert(word_inp): \n",
        "    return [ord(i) for i in word_inp] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oiAKv-SoLs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_seqs = [split_convert(i) for i in dat_train.iloc[:,0]]\n",
        "train_labels = [i for i in dat_train.iloc[:,1]]\n",
        "test_seqs = [split_convert(i) for i in dat_test.iloc[:,0]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfD9CYbP2cG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "train_seqs_onehot = onehot_encoder.fit_transform(train_seqs)\n",
        "test_seqs_onehot = onehot_encoder.transform(test_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_zRRJJ72q-K",
        "colab_type": "text"
      },
      "source": [
        "#### Define Neural Network Architecture and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDJA5RrP1FgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to determine metrics f1, precision and recall\n",
        "# taken from: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "\n",
        "def get_recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def get_precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def get_f1(y_true, y_pred):\n",
        "    precision = get_precision(y_true, y_pred)\n",
        "    recall = get_recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpI0G91aHgnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine class imbalance\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwlmVhLW07X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class_weight = {0:1, 1:12}\n",
        "NEPOCHS = 60    \n",
        "BATCHSIZE = 64\n",
        "VALIDATIONSPLIT = 0.2\n",
        "HIDDENSIZE = 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MLkZ2kHyfqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = 80, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dense(HIDDENSIZE, input_dim = HIDDENSIZE, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dense(2, input_dim = HIDDENSIZE, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USe8a-792wls",
        "colab_type": "text"
      },
      "source": [
        "#### Model Selection / training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmVPZ0er052Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold_splits = 5\n",
        "folds = list(StratifiedKFold(n_splits=kfold_splits, shuffle=True, random_state=1).split(train_seqs, train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd7SibE9ktB0",
        "colab_type": "code",
        "outputId": "eaa6ff08-04fc-4af5-8e70-5c272857f2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert labels to categorical one-hot encoding\n",
        "train_labels_onehot = keras.utils.to_categorical(train_labels, num_classes=2)\n",
        "\n",
        "mode_path = './models/mlp_v2.h5'\n",
        "\n",
        "model = None\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "best_fold = -1\n",
        "best_score = 0\n",
        "best_model = None\n",
        "\n",
        "for index, (train_indices, val_indices) in enumerate(folds):\n",
        "  print(\"Training on fold \" + str(index+1) + \"/5...\")\n",
        "  # Generate batches from indices\n",
        "  xtrain, xval = train_seqs_onehot[train_indices], train_seqs_onehot[val_indices]\n",
        "  ytrain, yval = train_labels_onehot[train_indices], train_labels_onehot[val_indices]\n",
        "\n",
        "  # xtrain_onehot = onehot_encoder.transform(xtrain)\n",
        "  # xval_onehot = onehot_encoder.transform(xval)\n",
        "  # ytrain_onehot = keras.utils.to_categorical(y_train, num_classes=2)\n",
        "  # yval_onehot = keras.utils.to_categorical(y_val, num_classes=2)\n",
        "\n",
        "  model = None\n",
        "  model = create_model()\n",
        "\n",
        "  # model.summary()\n",
        "  callbacks = [ModelCheckpoint(filepath=mode_path, save_best_only=True)]\n",
        "  model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = NEPOCHS, batch_size=BATCHSIZE, verbose = 1 ,\n",
        "            callbacks=callbacks, class_weight = class_weight_dict)  # starts training\n",
        "\n",
        "  # get the best fold based on the best f1 score\n",
        "  y_pred = model.predict(xval, batch_size=BATCHSIZE, verbose=1)\n",
        "  y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "  tmp_score = metrics.f1_score(np.argmax(yval, axis=1),y_pred_bool)\n",
        "  print(\"F1 score for this fold is : \", tmp_score)\n",
        "  if(tmp_score > best_score):\n",
        "    best_fold = index\n",
        "    best_model = model\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_209 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 80)                320       \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_216 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 80)                320       \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "dense_217 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_218 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_219 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dense_220 (Dense)            (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 80)                320       \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 2)                 162       \n",
            "=================================================================\n",
            "Total params: 78,882\n",
            "Trainable params: 78,402\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n",
            "Training on fold 1/5...\n",
            "Train on 89600 samples, validate on 22400 samples\n",
            "Epoch 1/60\n",
            "89600/89600 [==============================] - 11s 126us/step - loss: 0.4339 - accuracy: 0.7751 - val_loss: 0.1050 - val_accuracy: 0.9700\n",
            "Epoch 2/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2560 - accuracy: 0.9056 - val_loss: 0.2473 - val_accuracy: 0.9014\n",
            "Epoch 3/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2431 - accuracy: 0.9186 - val_loss: 0.1158 - val_accuracy: 0.9588\n",
            "Epoch 4/60\n",
            "89600/89600 [==============================] - 9s 96us/step - loss: 0.2453 - accuracy: 0.9137 - val_loss: 0.0860 - val_accuracy: 0.9655\n",
            "Epoch 5/60\n",
            "89600/89600 [==============================] - 9s 98us/step - loss: 0.2537 - accuracy: 0.9005 - val_loss: 0.0949 - val_accuracy: 0.9628\n",
            "Epoch 6/60\n",
            "89600/89600 [==============================] - 9s 98us/step - loss: 0.2405 - accuracy: 0.9047 - val_loss: 0.1662 - val_accuracy: 0.9506\n",
            "Epoch 7/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2380 - accuracy: 0.9081 - val_loss: 0.1260 - val_accuracy: 0.9612\n",
            "Epoch 8/60\n",
            "89600/89600 [==============================] - 9s 96us/step - loss: 0.2322 - accuracy: 0.9083 - val_loss: 0.1125 - val_accuracy: 0.9675\n",
            "Epoch 9/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2520 - accuracy: 0.8997 - val_loss: 0.3060 - val_accuracy: 0.9623\n",
            "Epoch 10/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2730 - accuracy: 0.8906 - val_loss: 0.1442 - val_accuracy: 0.9625\n",
            "Epoch 11/60\n",
            "89600/89600 [==============================] - 9s 99us/step - loss: 0.2833 - accuracy: 0.8814 - val_loss: 2.4094 - val_accuracy: 0.0801\n",
            "Epoch 12/60\n",
            "89600/89600 [==============================] - 9s 95us/step - loss: 0.2494 - accuracy: 0.9088 - val_loss: 0.9440 - val_accuracy: 0.5801\n",
            "Epoch 13/60\n",
            "89600/89600 [==============================] - 9s 99us/step - loss: 0.2436 - accuracy: 0.9033 - val_loss: 0.1248 - val_accuracy: 0.9618\n",
            "Epoch 14/60\n",
            "89600/89600 [==============================] - 9s 101us/step - loss: 0.2336 - accuracy: 0.9161 - val_loss: 0.0972 - val_accuracy: 0.9638\n",
            "Epoch 15/60\n",
            "89600/89600 [==============================] - 9s 100us/step - loss: 0.2371 - accuracy: 0.9118 - val_loss: 0.3780 - val_accuracy: 0.9629\n",
            "Epoch 16/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2315 - accuracy: 0.9150 - val_loss: 4.0241 - val_accuracy: 0.0378\n",
            "Epoch 17/60\n",
            "89600/89600 [==============================] - 9s 102us/step - loss: 0.2234 - accuracy: 0.9219 - val_loss: 0.0647 - val_accuracy: 0.9767\n",
            "Epoch 18/60\n",
            "89600/89600 [==============================] - 10s 106us/step - loss: 0.2184 - accuracy: 0.9197 - val_loss: 0.1941 - val_accuracy: 0.9668\n",
            "Epoch 19/60\n",
            "89600/89600 [==============================] - 13s 142us/step - loss: 0.2042 - accuracy: 0.9284 - val_loss: 0.1072 - val_accuracy: 0.9631\n",
            "Epoch 20/60\n",
            "89600/89600 [==============================] - 9s 99us/step - loss: 0.2375 - accuracy: 0.9147 - val_loss: 0.0586 - val_accuracy: 0.9785\n",
            "Epoch 21/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.2190 - accuracy: 0.9212 - val_loss: 0.1504 - val_accuracy: 0.9698\n",
            "Epoch 22/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2074 - accuracy: 0.9213 - val_loss: 1.1324 - val_accuracy: 0.3738\n",
            "Epoch 23/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2220 - accuracy: 0.9163 - val_loss: 0.0868 - val_accuracy: 0.9736\n",
            "Epoch 24/60\n",
            "89600/89600 [==============================] - 9s 97us/step - loss: 0.2225 - accuracy: 0.9170 - val_loss: 0.1707 - val_accuracy: 0.9347\n",
            "Epoch 25/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.2046 - accuracy: 0.9277 - val_loss: 0.4750 - val_accuracy: 0.7980\n",
            "Epoch 26/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2024 - accuracy: 0.9298 - val_loss: 0.1015 - val_accuracy: 0.9609\n",
            "Epoch 27/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2034 - accuracy: 0.9282 - val_loss: 1.0031 - val_accuracy: 0.4414\n",
            "Epoch 28/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.2097 - accuracy: 0.9312 - val_loss: 2.7948 - val_accuracy: 0.0967\n",
            "Epoch 29/60\n",
            "89600/89600 [==============================] - 9s 99us/step - loss: 0.2020 - accuracy: 0.9265 - val_loss: 0.2315 - val_accuracy: 0.9679\n",
            "Epoch 30/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2034 - accuracy: 0.9310 - val_loss: 0.1923 - val_accuracy: 0.9702\n",
            "Epoch 31/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.2027 - accuracy: 0.9301 - val_loss: 0.2564 - val_accuracy: 0.9114\n",
            "Epoch 32/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1988 - accuracy: 0.9303 - val_loss: 1.4529 - val_accuracy: 0.3183\n",
            "Epoch 33/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2018 - accuracy: 0.9320 - val_loss: 0.2472 - val_accuracy: 0.9215\n",
            "Epoch 34/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2052 - accuracy: 0.9328 - val_loss: 0.0767 - val_accuracy: 0.9784\n",
            "Epoch 35/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.1934 - accuracy: 0.9349 - val_loss: 0.2541 - val_accuracy: 0.8889\n",
            "Epoch 36/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1925 - accuracy: 0.9351 - val_loss: 0.3398 - val_accuracy: 0.8744\n",
            "Epoch 37/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1953 - accuracy: 0.9369 - val_loss: 0.1576 - val_accuracy: 0.9344\n",
            "Epoch 38/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1958 - accuracy: 0.9367 - val_loss: 0.6414 - val_accuracy: 0.7454\n",
            "Epoch 39/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1997 - accuracy: 0.9331 - val_loss: 0.1050 - val_accuracy: 0.9580\n",
            "Epoch 40/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1972 - accuracy: 0.9338 - val_loss: 0.0918 - val_accuracy: 0.9723\n",
            "Epoch 41/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1895 - accuracy: 0.9349 - val_loss: 0.1897 - val_accuracy: 0.9288\n",
            "Epoch 42/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1883 - accuracy: 0.9384 - val_loss: 0.1058 - val_accuracy: 0.9668\n",
            "Epoch 43/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.1838 - accuracy: 0.9421 - val_loss: 0.9024 - val_accuracy: 0.5925\n",
            "Epoch 44/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1840 - accuracy: 0.9386 - val_loss: 0.0980 - val_accuracy: 0.9754\n",
            "Epoch 45/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1829 - accuracy: 0.9408 - val_loss: 2.6652 - val_accuracy: 0.0622\n",
            "Epoch 46/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1938 - accuracy: 0.9361 - val_loss: 0.0595 - val_accuracy: 0.9799\n",
            "Epoch 47/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1856 - accuracy: 0.9394 - val_loss: 0.0978 - val_accuracy: 0.9591\n",
            "Epoch 48/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1886 - accuracy: 0.9349 - val_loss: 0.0854 - val_accuracy: 0.9736\n",
            "Epoch 49/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1927 - accuracy: 0.9329 - val_loss: 0.1432 - val_accuracy: 0.9439\n",
            "Epoch 50/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1912 - accuracy: 0.9350 - val_loss: 0.0983 - val_accuracy: 0.9615\n",
            "Epoch 51/60\n",
            "89600/89600 [==============================] - 8s 84us/step - loss: 0.1852 - accuracy: 0.9365 - val_loss: 1.6026 - val_accuracy: 0.2369\n",
            "Epoch 52/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1854 - accuracy: 0.9390 - val_loss: 0.0777 - val_accuracy: 0.9733\n",
            "Epoch 53/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1827 - accuracy: 0.9406 - val_loss: 0.1012 - val_accuracy: 0.9611\n",
            "Epoch 54/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1872 - accuracy: 0.9369 - val_loss: 0.1317 - val_accuracy: 0.9535\n",
            "Epoch 55/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1752 - accuracy: 0.9417 - val_loss: 0.0621 - val_accuracy: 0.9760\n",
            "Epoch 56/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.1851 - accuracy: 0.9366 - val_loss: 0.1052 - val_accuracy: 0.9641\n",
            "Epoch 57/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1803 - accuracy: 0.9404 - val_loss: 0.0739 - val_accuracy: 0.9700\n",
            "Epoch 58/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1852 - accuracy: 0.9372 - val_loss: 1.3358 - val_accuracy: 0.2842\n",
            "Epoch 59/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1803 - accuracy: 0.9429 - val_loss: 0.0581 - val_accuracy: 0.9790\n",
            "Epoch 60/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1814 - accuracy: 0.9370 - val_loss: 0.3074 - val_accuracy: 0.8907\n",
            "22400/22400 [==============================] - 2s 104us/step\n",
            "F1 score for this fold is :  0.40572676534821644\n",
            "Training on fold 2/5...\n",
            "Train on 89600 samples, validate on 22400 samples\n",
            "Epoch 1/60\n",
            "89600/89600 [==============================] - 10s 111us/step - loss: 0.4413 - accuracy: 0.7797 - val_loss: 0.0898 - val_accuracy: 0.9694\n",
            "Epoch 2/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2662 - accuracy: 0.9091 - val_loss: 0.1393 - val_accuracy: 0.9498\n",
            "Epoch 3/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2314 - accuracy: 0.9208 - val_loss: 0.2542 - val_accuracy: 0.8882\n",
            "Epoch 4/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.2268 - accuracy: 0.9218 - val_loss: 0.3136 - val_accuracy: 0.8963\n",
            "Epoch 5/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2255 - accuracy: 0.9261 - val_loss: 0.3233 - val_accuracy: 0.8894\n",
            "Epoch 6/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.2348 - accuracy: 0.9161 - val_loss: 0.2260 - val_accuracy: 0.9591\n",
            "Epoch 7/60\n",
            "89600/89600 [==============================] - 9s 98us/step - loss: 0.2226 - accuracy: 0.9151 - val_loss: 0.2290 - val_accuracy: 0.9201\n",
            "Epoch 8/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2377 - accuracy: 0.9068 - val_loss: 0.1510 - val_accuracy: 0.9377\n",
            "Epoch 9/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2585 - accuracy: 0.8956 - val_loss: 0.0965 - val_accuracy: 0.9664\n",
            "Epoch 10/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2619 - accuracy: 0.8917 - val_loss: 0.5535 - val_accuracy: 0.9624\n",
            "Epoch 11/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2357 - accuracy: 0.9078 - val_loss: 0.1115 - val_accuracy: 0.9511\n",
            "Epoch 12/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2393 - accuracy: 0.9100 - val_loss: 1.6289 - val_accuracy: 0.5384\n",
            "Epoch 13/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2372 - accuracy: 0.9131 - val_loss: 0.1615 - val_accuracy: 0.9375\n",
            "Epoch 14/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.2226 - accuracy: 0.9157 - val_loss: 0.5313 - val_accuracy: 0.7601\n",
            "Epoch 15/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2211 - accuracy: 0.9151 - val_loss: 0.0569 - val_accuracy: 0.9789\n",
            "Epoch 16/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2162 - accuracy: 0.9222 - val_loss: 0.1774 - val_accuracy: 0.9656\n",
            "Epoch 17/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2237 - accuracy: 0.9161 - val_loss: 0.0899 - val_accuracy: 0.9694\n",
            "Epoch 18/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2340 - accuracy: 0.9149 - val_loss: 0.4299 - val_accuracy: 0.8410\n",
            "Epoch 19/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2182 - accuracy: 0.9212 - val_loss: 0.1093 - val_accuracy: 0.9616\n",
            "Epoch 20/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2010 - accuracy: 0.9328 - val_loss: 0.0931 - val_accuracy: 0.9695\n",
            "Epoch 21/60\n",
            "89600/89600 [==============================] - 7s 84us/step - loss: 0.1968 - accuracy: 0.9314 - val_loss: 0.5387 - val_accuracy: 0.8180\n",
            "Epoch 22/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2028 - accuracy: 0.9342 - val_loss: 0.3293 - val_accuracy: 0.8921\n",
            "Epoch 23/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.2013 - accuracy: 0.9334 - val_loss: 0.1062 - val_accuracy: 0.9680\n",
            "Epoch 24/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1906 - accuracy: 0.9376 - val_loss: 0.1433 - val_accuracy: 0.9419\n",
            "Epoch 25/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.2005 - accuracy: 0.9375 - val_loss: 0.1132 - val_accuracy: 0.9600\n",
            "Epoch 26/60\n",
            "89600/89600 [==============================] - 8s 84us/step - loss: 0.2003 - accuracy: 0.9327 - val_loss: 0.1689 - val_accuracy: 0.9413\n",
            "Epoch 27/60\n",
            "89600/89600 [==============================] - 8s 84us/step - loss: 0.1904 - accuracy: 0.9367 - val_loss: 0.0810 - val_accuracy: 0.9679\n",
            "Epoch 28/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1846 - accuracy: 0.9392 - val_loss: 0.2683 - val_accuracy: 0.8675\n",
            "Epoch 29/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1886 - accuracy: 0.9356 - val_loss: 0.2241 - val_accuracy: 0.9678\n",
            "Epoch 30/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1852 - accuracy: 0.9417 - val_loss: 0.0584 - val_accuracy: 0.9805\n",
            "Epoch 31/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1946 - accuracy: 0.9358 - val_loss: 0.0871 - val_accuracy: 0.9662\n",
            "Epoch 32/60\n",
            "89600/89600 [==============================] - 11s 123us/step - loss: 0.1912 - accuracy: 0.9376 - val_loss: 0.0564 - val_accuracy: 0.9787\n",
            "Epoch 33/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1886 - accuracy: 0.9377 - val_loss: 0.7264 - val_accuracy: 0.7325\n",
            "Epoch 34/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1892 - accuracy: 0.9371 - val_loss: 0.0792 - val_accuracy: 0.9758\n",
            "Epoch 35/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1893 - accuracy: 0.9359 - val_loss: 0.1449 - val_accuracy: 0.9502\n",
            "Epoch 36/60\n",
            "89600/89600 [==============================] - 8s 84us/step - loss: 0.1784 - accuracy: 0.9417 - val_loss: 0.1573 - val_accuracy: 0.9534\n",
            "Epoch 37/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1792 - accuracy: 0.9377 - val_loss: 0.1356 - val_accuracy: 0.9450\n",
            "Epoch 38/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1936 - accuracy: 0.9374 - val_loss: 0.0490 - val_accuracy: 0.9812\n",
            "Epoch 39/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1907 - accuracy: 0.9376 - val_loss: 0.0688 - val_accuracy: 0.9778\n",
            "Epoch 40/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1876 - accuracy: 0.9352 - val_loss: 0.1109 - val_accuracy: 0.9566\n",
            "Epoch 41/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2005 - accuracy: 0.9300 - val_loss: 0.7859 - val_accuracy: 0.6076\n",
            "Epoch 42/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1967 - accuracy: 0.9315 - val_loss: 0.1396 - val_accuracy: 0.9493\n",
            "Epoch 43/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1937 - accuracy: 0.9345 - val_loss: 0.0727 - val_accuracy: 0.9778\n",
            "Epoch 44/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1873 - accuracy: 0.9349 - val_loss: 0.0782 - val_accuracy: 0.9797\n",
            "Epoch 45/60\n",
            "89600/89600 [==============================] - 8s 84us/step - loss: 0.1884 - accuracy: 0.9389 - val_loss: 0.1321 - val_accuracy: 0.9475\n",
            "Epoch 46/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1814 - accuracy: 0.9380 - val_loss: 0.0857 - val_accuracy: 0.9722\n",
            "Epoch 47/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1841 - accuracy: 0.9377 - val_loss: 0.8659 - val_accuracy: 0.5671\n",
            "Epoch 48/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1888 - accuracy: 0.9403 - val_loss: 0.0822 - val_accuracy: 0.9643\n",
            "Epoch 49/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1859 - accuracy: 0.9399 - val_loss: 0.0614 - val_accuracy: 0.9787\n",
            "Epoch 50/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1837 - accuracy: 0.9404 - val_loss: 0.5942 - val_accuracy: 0.7581\n",
            "Epoch 51/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1832 - accuracy: 0.9394 - val_loss: 0.0584 - val_accuracy: 0.9794\n",
            "Epoch 52/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1876 - accuracy: 0.9388 - val_loss: 0.1958 - val_accuracy: 0.9686\n",
            "Epoch 53/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1921 - accuracy: 0.9371 - val_loss: 0.4587 - val_accuracy: 0.8299\n",
            "Epoch 54/60\n",
            "89600/89600 [==============================] - 7s 83us/step - loss: 0.1853 - accuracy: 0.9366 - val_loss: 0.0858 - val_accuracy: 0.9662\n",
            "Epoch 55/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1811 - accuracy: 0.9427 - val_loss: 0.0571 - val_accuracy: 0.9825\n",
            "Epoch 56/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1809 - accuracy: 0.9387 - val_loss: 0.0488 - val_accuracy: 0.9818\n",
            "Epoch 57/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1917 - accuracy: 0.9366 - val_loss: 0.3050 - val_accuracy: 0.8892\n",
            "Epoch 58/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1890 - accuracy: 0.9391 - val_loss: 0.8989 - val_accuracy: 0.6665\n",
            "Epoch 59/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1952 - accuracy: 0.9356 - val_loss: 0.1396 - val_accuracy: 0.9549\n",
            "Epoch 60/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1908 - accuracy: 0.9331 - val_loss: 0.0966 - val_accuracy: 0.9747\n",
            "22400/22400 [==============================] - 2s 106us/step\n",
            "F1 score for this fold is :  0.5255230125523013\n",
            "Training on fold 3/5...\n",
            "Train on 89600 samples, validate on 22400 samples\n",
            "Epoch 1/60\n",
            "89600/89600 [==============================] - 10s 112us/step - loss: 0.4234 - accuracy: 0.7769 - val_loss: 0.1695 - val_accuracy: 0.9304\n",
            "Epoch 2/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2606 - accuracy: 0.9109 - val_loss: 0.0956 - val_accuracy: 0.9671\n",
            "Epoch 3/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.2308 - accuracy: 0.9205 - val_loss: 0.1782 - val_accuracy: 0.9308\n",
            "Epoch 4/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.2127 - accuracy: 0.9269 - val_loss: 0.1208 - val_accuracy: 0.9700\n",
            "Epoch 5/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2082 - accuracy: 0.9262 - val_loss: 0.1421 - val_accuracy: 0.9413\n",
            "Epoch 6/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.2143 - accuracy: 0.9282 - val_loss: 0.1183 - val_accuracy: 0.9641\n",
            "Epoch 7/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.2123 - accuracy: 0.9180 - val_loss: 0.2780 - val_accuracy: 0.8863\n",
            "Epoch 8/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2096 - accuracy: 0.9243 - val_loss: 0.4980 - val_accuracy: 0.7953\n",
            "Epoch 9/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2073 - accuracy: 0.9241 - val_loss: 0.0726 - val_accuracy: 0.9720\n",
            "Epoch 10/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.2046 - accuracy: 0.9225 - val_loss: 0.1237 - val_accuracy: 0.9631\n",
            "Epoch 11/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.2070 - accuracy: 0.9224 - val_loss: 0.1543 - val_accuracy: 0.9439\n",
            "Epoch 12/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1944 - accuracy: 0.9340 - val_loss: 0.4593 - val_accuracy: 0.8321\n",
            "Epoch 13/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1831 - accuracy: 0.9360 - val_loss: 0.2147 - val_accuracy: 0.9139\n",
            "Epoch 14/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1820 - accuracy: 0.9357 - val_loss: 1.9508 - val_accuracy: 0.3539\n",
            "Epoch 15/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1842 - accuracy: 0.9360 - val_loss: 0.0721 - val_accuracy: 0.9750\n",
            "Epoch 16/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1839 - accuracy: 0.9360 - val_loss: 0.3175 - val_accuracy: 0.8954\n",
            "Epoch 17/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1786 - accuracy: 0.9396 - val_loss: 0.0671 - val_accuracy: 0.9736\n",
            "Epoch 18/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1920 - accuracy: 0.9336 - val_loss: 0.1932 - val_accuracy: 0.9263\n",
            "Epoch 19/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1768 - accuracy: 0.9430 - val_loss: 0.1027 - val_accuracy: 0.9667\n",
            "Epoch 20/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1838 - accuracy: 0.9419 - val_loss: 0.1086 - val_accuracy: 0.9661\n",
            "Epoch 21/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1822 - accuracy: 0.9389 - val_loss: 0.1642 - val_accuracy: 0.9581\n",
            "Epoch 22/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1831 - accuracy: 0.9408 - val_loss: 0.0964 - val_accuracy: 0.9688\n",
            "Epoch 23/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.1793 - accuracy: 0.9422 - val_loss: 0.1269 - val_accuracy: 0.9567\n",
            "Epoch 24/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1778 - accuracy: 0.9432 - val_loss: 0.0520 - val_accuracy: 0.9817\n",
            "Epoch 25/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1790 - accuracy: 0.9456 - val_loss: 0.1913 - val_accuracy: 0.9529\n",
            "Epoch 26/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1831 - accuracy: 0.9439 - val_loss: 0.0842 - val_accuracy: 0.9693\n",
            "Epoch 27/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1740 - accuracy: 0.9447 - val_loss: 0.1048 - val_accuracy: 0.9729\n",
            "Epoch 28/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1679 - accuracy: 0.9447 - val_loss: 0.0853 - val_accuracy: 0.9751\n",
            "Epoch 29/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1748 - accuracy: 0.9435 - val_loss: 0.7976 - val_accuracy: 0.6627\n",
            "Epoch 30/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.1731 - accuracy: 0.9459 - val_loss: 0.3366 - val_accuracy: 0.8772\n",
            "Epoch 31/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1684 - accuracy: 0.9442 - val_loss: 0.1085 - val_accuracy: 0.9691\n",
            "Epoch 32/60\n",
            "89600/89600 [==============================] - 8s 85us/step - loss: 0.1684 - accuracy: 0.9450 - val_loss: 0.2532 - val_accuracy: 0.9153\n",
            "Epoch 33/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1649 - accuracy: 0.9474 - val_loss: 0.1331 - val_accuracy: 0.9421\n",
            "Epoch 34/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1666 - accuracy: 0.9472 - val_loss: 0.0986 - val_accuracy: 0.9714\n",
            "Epoch 35/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1674 - accuracy: 0.9446 - val_loss: 0.1004 - val_accuracy: 0.9638\n",
            "Epoch 36/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1661 - accuracy: 0.9461 - val_loss: 0.1870 - val_accuracy: 0.9646\n",
            "Epoch 37/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1709 - accuracy: 0.9416 - val_loss: 0.0993 - val_accuracy: 0.9650\n",
            "Epoch 38/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1665 - accuracy: 0.9437 - val_loss: 0.1176 - val_accuracy: 0.9586\n",
            "Epoch 39/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1691 - accuracy: 0.9412 - val_loss: 5.2829 - val_accuracy: 0.1579\n",
            "Epoch 40/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1747 - accuracy: 0.9388 - val_loss: 0.0470 - val_accuracy: 0.9814\n",
            "Epoch 41/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1703 - accuracy: 0.9412 - val_loss: 0.0736 - val_accuracy: 0.9734\n",
            "Epoch 42/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1740 - accuracy: 0.9414 - val_loss: 0.1724 - val_accuracy: 0.9718\n",
            "Epoch 43/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1710 - accuracy: 0.9414 - val_loss: 0.1540 - val_accuracy: 0.9416\n",
            "Epoch 44/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1764 - accuracy: 0.9369 - val_loss: 0.1804 - val_accuracy: 0.9454\n",
            "Epoch 45/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1735 - accuracy: 0.9395 - val_loss: 0.1861 - val_accuracy: 0.9700\n",
            "Epoch 46/60\n",
            "89600/89600 [==============================] - 10s 116us/step - loss: 0.1753 - accuracy: 0.9415 - val_loss: 0.2447 - val_accuracy: 0.9085\n",
            "Epoch 47/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.1776 - accuracy: 0.9407 - val_loss: 0.1581 - val_accuracy: 0.9292\n",
            "Epoch 48/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1863 - accuracy: 0.9422 - val_loss: 0.1210 - val_accuracy: 0.9596\n",
            "Epoch 49/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1746 - accuracy: 0.9403 - val_loss: 0.1380 - val_accuracy: 0.9442\n",
            "Epoch 50/60\n",
            "89600/89600 [==============================] - 8s 87us/step - loss: 0.1741 - accuracy: 0.9448 - val_loss: 0.0655 - val_accuracy: 0.9808\n",
            "Epoch 51/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1722 - accuracy: 0.9444 - val_loss: 0.1683 - val_accuracy: 0.9374\n",
            "Epoch 52/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1752 - accuracy: 0.9438 - val_loss: 1.7092 - val_accuracy: 0.3138\n",
            "Epoch 53/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1829 - accuracy: 0.9424 - val_loss: 0.1353 - val_accuracy: 0.9563\n",
            "Epoch 54/60\n",
            "89600/89600 [==============================] - 8s 86us/step - loss: 0.1691 - accuracy: 0.9426 - val_loss: 0.0555 - val_accuracy: 0.9833\n",
            "Epoch 55/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1683 - accuracy: 0.9463 - val_loss: 0.0618 - val_accuracy: 0.9812\n",
            "Epoch 56/60\n",
            "89600/89600 [==============================] - 8s 89us/step - loss: 0.1705 - accuracy: 0.9446 - val_loss: 0.0679 - val_accuracy: 0.9791\n",
            "Epoch 57/60\n",
            "89600/89600 [==============================] - 8s 88us/step - loss: 0.1728 - accuracy: 0.9426 - val_loss: 0.2926 - val_accuracy: 0.9155\n",
            "Epoch 58/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1745 - accuracy: 0.9429 - val_loss: 0.2895 - val_accuracy: 0.9641\n",
            "Epoch 59/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.1730 - accuracy: 0.9433 - val_loss: 0.0628 - val_accuracy: 0.9795\n",
            "Epoch 60/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1695 - accuracy: 0.9458 - val_loss: 0.1242 - val_accuracy: 0.9692\n",
            "22400/22400 [==============================] - 2s 111us/step\n",
            "F1 score for this fold is :  0.30969030969030964\n",
            "Training on fold 4/5...\n",
            "Train on 89600 samples, validate on 22400 samples\n",
            "Epoch 1/60\n",
            "89600/89600 [==============================] - 10s 115us/step - loss: 0.4651 - accuracy: 0.7476 - val_loss: 0.0953 - val_accuracy: 0.9700\n",
            "Epoch 2/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.2772 - accuracy: 0.9097 - val_loss: 0.0892 - val_accuracy: 0.9667\n",
            "Epoch 3/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.2404 - accuracy: 0.9174 - val_loss: 0.0846 - val_accuracy: 0.9674\n",
            "Epoch 4/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.2342 - accuracy: 0.9174 - val_loss: 0.1261 - val_accuracy: 0.9483\n",
            "Epoch 5/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.2404 - accuracy: 0.9079 - val_loss: 0.1430 - val_accuracy: 0.9455\n",
            "Epoch 6/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.2315 - accuracy: 0.9131 - val_loss: 0.2005 - val_accuracy: 0.9243\n",
            "Epoch 7/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.2345 - accuracy: 0.9109 - val_loss: 0.1041 - val_accuracy: 0.9575\n",
            "Epoch 8/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.2252 - accuracy: 0.9128 - val_loss: 0.3469 - val_accuracy: 0.9625\n",
            "Epoch 9/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2331 - accuracy: 0.9120 - val_loss: 0.3014 - val_accuracy: 0.9624\n",
            "Epoch 10/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2170 - accuracy: 0.9187 - val_loss: 0.0734 - val_accuracy: 0.9750\n",
            "Epoch 11/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2128 - accuracy: 0.9225 - val_loss: 0.1432 - val_accuracy: 0.9520\n",
            "Epoch 12/60\n",
            "89600/89600 [==============================] - 9s 96us/step - loss: 0.2099 - accuracy: 0.9207 - val_loss: 0.2435 - val_accuracy: 0.9243\n",
            "Epoch 13/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.2054 - accuracy: 0.9264 - val_loss: 0.2052 - val_accuracy: 0.9633\n",
            "Epoch 14/60\n",
            "89600/89600 [==============================] - 8s 91us/step - loss: 0.2057 - accuracy: 0.9301 - val_loss: 0.0678 - val_accuracy: 0.9742\n",
            "Epoch 15/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.2059 - accuracy: 0.9272 - val_loss: 0.0721 - val_accuracy: 0.9772\n",
            "Epoch 16/60\n",
            "89600/89600 [==============================] - 8s 92us/step - loss: 0.1952 - accuracy: 0.9302 - val_loss: 0.2361 - val_accuracy: 0.9273\n",
            "Epoch 17/60\n",
            "89600/89600 [==============================] - 8s 93us/step - loss: 0.1986 - accuracy: 0.9288 - val_loss: 1.2798 - val_accuracy: 0.4957\n",
            "Epoch 18/60\n",
            "89600/89600 [==============================] - 8s 90us/step - loss: 0.1882 - accuracy: 0.9371 - val_loss: 1.4032 - val_accuracy: 0.4520\n",
            "Epoch 19/60\n",
            "89600/89600 [==============================] - 8s 94us/step - loss: 0.1769 - accuracy: 0.9376 - val_loss: 0.6285 - val_accuracy: 0.7314\n",
            "Epoch 20/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-4764228a83b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   model.fit(xtrain, ytrain, validation_data = (xval, yval), epochs = NEPOCHS, batch_size=BATCHSIZE, verbose = 1 ,\n\u001b[0;32m---> 30\u001b[0;31m             callbacks=callbacks, class_weight = class_weight_dict)  # starts training\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# get the best fold based on the best f1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mbarstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%%%dd/%d ['\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m                 \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbarstr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qR1ILk9sWm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Error\n",
        "y_pred = best_model.predict(train_seqs_onehot, batch_size=BATCHSIZE, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(train_labels, y_pred_bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wULYMI8woU",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUlOIEMzxP6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = best_model.predict(test_seqs_onehot, batch_size=BATCHSIZE,verbose = 1)\n",
        "res = np.argmax(y_pred, axis=1)\n",
        "print(np.sum(res))\n",
        "\n",
        "res = pd.DataFrame(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OttnRkXZ8zil",
        "colab_type": "text"
      },
      "source": [
        "#### Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux7REfQX0Uhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.to_csv(\"./prediction.csv\", index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Przd4rnpTGxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}