{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, mutual_info_classif, mutual_info_regression, f_regression\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "# load data from csv file\n",
    "df_train_features = pd.read_csv ('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Load test data\n",
    "df_test_features = pd.read_csv ('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels = df_train_labels.sort_values('pid')\n",
    "df_train_features = df_train_features.sort_values(['pid', 'Time'])\n",
    "\n",
    "# Droping time\n",
    "df_train_features = df_train_features.drop('Time', axis = 1)\n",
    "df_test_features = df_test_features.drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Histogram of the output labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.hist()\n",
    "\n",
    "# with PdfPages(\"./Results/Labels_histogram.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see the class imbalance problem here. Other observations:\n",
    "  * Heartrate, RRate, ABPm,  distribution is similar to a normal distribution\n",
    "  * SpO2 is like a censored normal distribution. \n",
    "  * For all of the other features, class imbalance is an obvious problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic strategy that could be used here: Upsample both classes! Do the upsampling efficiently, not just replicating the datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot over features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inspection: \n",
    "#############################################\n",
    "# range of the provided data?\n",
    "print(df_train_features.agg([min, max]))\n",
    "\n",
    "# Boxplotting the data\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.set_title('BUN')\n",
    "# ax2.boxplot(df_train_features.iloc[:,5], notch=True)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = df_train_features.iloc[:,1:])\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "# with PdfPages(\"./Results/Train_columns_boxplot.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = df_train_features.corr()\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, \n",
    "        vmin=-1, vmax=1, center=0, \n",
    "           cmap=sns.diverging_palette(20, 220, n=200))\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing pattern of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much missing data? \n",
    "print(\"Percentage of missing values:\")\n",
    "print(df_train_features.isnull().sum(axis=0) / len(df_train_features))\n",
    "\n",
    "msno.matrix(df_train_features)\n",
    "\n",
    "# Plotting the correlation between the missing values\n",
    "msno.heatmap(df_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  6 10 20 21 24 27 31 34]\n",
      "[ 7 15 17 23 30 35]\n",
      "[ 2  3  4  5  8  9 11 12 13 14 16 18 19 22 25 26 28 29 32 33]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'EtCO2', 'PTT', 'BUN', 'Lactate', 'Temp', 'Hgb', 'HCO3',\n",
       "       'BaseExcess', 'RRate', 'Fibrinogen', 'Phosphate', 'WBC', 'Creatinine',\n",
       "       'PaCO2', 'AST', 'FiO2', 'Platelets', 'SaO2', 'Glucose', 'ABPm',\n",
       "       'Magnesium', 'Potassium', 'ABPd', 'Calcium', 'Alkalinephos', 'SpO2',\n",
       "       'Bilirubin_direct', 'Chloride', 'Hct', 'Heartrate', 'Bilirubin_total',\n",
       "       'TroponinI', 'ABPs', 'pH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which columns have less than a percent NA\n",
    "NA_percent = 0.8\n",
    "NA_percent_severe = 0.91\n",
    "\n",
    "sel_features = df_train_features.isnull().sum(axis=0) < (NA_percent * df_train_features.shape[0])\n",
    "inds = np.where(sel_features == True)\n",
    "\n",
    "sel_features_2 = (df_train_features.isnull().sum(axis=0) < (NA_percent_severe * df_train_features.shape[0])) & (df_train_features.isnull().sum(axis=0) > (NA_percent * df_train_features.shape[0]))        \n",
    "inds_2 = np.where(sel_features_2 == True)\n",
    "\n",
    "sel_features_3 = df_train_features.isnull().sum(axis=0) > (NA_percent_severe * df_train_features.shape[0])\n",
    "inds_3 = np.where(sel_features_3 == True)\n",
    "\n",
    "print(inds[0])\n",
    "print(inds_2[0])\n",
    "print(inds_3[0])\n",
    "vars_class1 = df_train_features.columns[inds[0]]\n",
    "vars_class2 = df_train_features.columns[inds_2[0]]\n",
    "vars_class3 = df_train_features.columns[inds_3[0]]\n",
    "vars_classes = [vars_class1, vars_class2, vars_class3]\n",
    "\n",
    "df_train_features.columns.drop(\"pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: All-NaN axis encountered\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: All-NaN axis encountered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def aggregation_technique(dat, variables, pids, vars_classes):\n",
    "    \n",
    "    vars_class1 = vars_classes[0]\n",
    "    vars_class2 = vars_classes[1]\n",
    "    vars_class3 = vars_classes[2]\n",
    "    \n",
    "    aggregated_dataframe = []\n",
    "    for pid in pids:\n",
    "        # dataframe for this pid\n",
    "        df = dat.loc[dat['pid'] == pid]\n",
    "#         calculated_features = [pid]\n",
    "        for var in variables:\n",
    "            # how many NaN are in there for this variable\n",
    "            data = df[var].tolist()\n",
    "#             num_nan = np.count_nonzero(np.isnan(data))\n",
    "            if var == \"Age\":\n",
    "                calculated_features.append(data[0])\n",
    "                \n",
    "            elif var in vars_class1:\n",
    "                calculated_features = calculated_features + data\n",
    "#                 calculated_features.append(np.nanmean(data))\n",
    "#                 calculated_features.append(np.nanstd(data))\n",
    "                \n",
    "            elif var in vars_class2:\n",
    "                calculated_features.append(np.nanmean(data))\n",
    "                calculated_features.append(np.nanstd(data))\n",
    "                calculated_features.append(np.nanmin(data))\n",
    "                calculated_features.append(np.nanmax(data))\n",
    "            else:\n",
    "                calculated_features.append(np.nanmean(data))\n",
    "                calculated_features.append(np.nanstd(data))\n",
    "                \n",
    "        aggregated_dataframe.append(calculated_features)\n",
    "        print(len(calculated_features))\n",
    "    aggregated_dataframe = pd.DataFrame(aggregated_dataframe)\n",
    "    return(aggregated_dataframe)\n",
    " \n",
    "train_pids = df_train_features['pid'].unique()\n",
    "df_train_agg_features = aggregation_technique(df_train_features, df_train_features.columns.drop(\"pid\"), train_pids, vars_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 162)\n",
      "RangeIndex(start=0, stop=162, step=1)\n",
      "         0     1    2    3     4    5     6    7    8    9    ...    152  \\\n",
      "0          1  34.0  NaN  NaN   NaN  NaN  12.0  0.0  NaN  NaN  ...  112.0   \n",
      "1          2  86.0  NaN  NaN  31.8  0.0  32.0  0.0  NaN  NaN  ...  138.0   \n",
      "2          4  66.0  NaN  NaN  34.6  0.0   8.0  0.0  NaN  NaN  ...  130.0   \n",
      "3          6  66.0  NaN  NaN  53.8  0.0  32.0  0.0  1.8  0.0  ...  100.0   \n",
      "4          8  42.0  NaN  NaN   NaN  NaN  18.0  0.0  NaN  NaN  ...  229.0   \n",
      "...      ...   ...  ...  ...   ...  ...   ...  ...  ...  ...  ...    ...   \n",
      "18990  31653  52.0  NaN  NaN  25.8  0.0  11.0  0.0  1.7  0.4  ...    NaN   \n",
      "18991  31654  66.0  NaN  NaN   NaN  NaN  33.0  1.0  NaN  NaN  ...  176.0   \n",
      "18992  31656  44.0  NaN  NaN   NaN  NaN  15.0  0.0  NaN  NaN  ...   88.0   \n",
      "18993  31657  70.0  NaN  NaN   NaN  NaN   NaN  NaN  NaN  NaN  ...  106.0   \n",
      "18994  31658  60.0  NaN  NaN   NaN  NaN  13.0  0.0  NaN  NaN  ...  127.0   \n",
      "\n",
      "         153    154    155    156    157       158       159   160   161  \n",
      "0      121.0  118.0  110.0  124.0  102.0  7.370000  0.028284  7.33  7.41  \n",
      "1      132.0  125.0  129.0  126.0  136.0       NaN       NaN   NaN   NaN  \n",
      "2      116.0   98.0   86.0   90.0   96.0       NaN       NaN   NaN   NaN  \n",
      "3       98.0  110.0   94.0  108.0   98.0  7.352857  0.019060  7.33  7.39  \n",
      "4      190.0  178.0  183.0  209.0  204.0       NaN       NaN   NaN   NaN  \n",
      "...      ...    ...    ...    ...    ...       ...       ...   ...   ...  \n",
      "18990   85.0   94.0    NaN   88.0    NaN  7.330000  0.000000  7.33  7.33  \n",
      "18991  187.0  175.0  156.0  157.0  153.0       NaN       NaN   NaN   NaN  \n",
      "18992  120.0   91.0   93.0  104.0    NaN  7.300000  0.036742  7.25  7.34  \n",
      "18993    NaN  101.0  109.0  109.0  106.0       NaN       NaN   NaN   NaN  \n",
      "18994  127.0    NaN    NaN  135.0  135.0       NaN       NaN   NaN   NaN  \n",
      "\n",
      "[18995 rows x 162 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train_agg_features.shape)\n",
    "print(df_train_agg_features.columns)\n",
    "print(df_train_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the imputer with a simple Random Forest estimator\n",
    "# imp = IterativeImputer(RandomForestRegressor(n_estimators=5), max_iter=10, random_state=1)\n",
    "\n",
    "# #perform filling\n",
    "# df_train_agg_imputed_features = pd.DataFrame(imp.fit_transform(df_train_agg_features), columns=df_train_agg_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "#imp = SimpleImputer(strategy=\"mean\")\n",
    "imputer = KNNImputer(n_neighbors = 5)\n",
    "\n",
    "df_train_agg_imputed_features = imputer.fit_transform(df_train_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "# standard_scalar = preprocessing.StandardScaler()\n",
    "\n",
    "data_train_scaled = min_max_scaler.fit_transform(df_train_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training data after imputing and aggregating\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = pd.DataFrame(data_train_scaled))\n",
    "ax.set_xticklabels(\n",
    "    list(df_train_features),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the correlation between the \n",
    "pd.DataFrame(data_train_scaled).corrwith(other = pd.DataFrame(df_train_agg_imputed_features), method = \"spearman\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(data_train_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, df_train_labels[[df_train_labels.columns[11]]]], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA for label i', fontsize = 20)\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[df_train_labels.columns[11]] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1667: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: All-NaN axis encountered\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: All-NaN axis encountered\n"
     ]
    }
   ],
   "source": [
    "test_pids = df_test_features['pid'].unique()\n",
    "df_test_agg_features = aggregation_technique(df_test_features, df_test_features.columns.drop(\"pid\"),\n",
    "                                             test_pids, vars_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "# should we impute it with the same imputer that we've used for train?\n",
    "\n",
    "imputer = KNNImputer(n_neighbors= 5)\n",
    "df_test_agg_imputed_features = imputer.fit_transform(df_test_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "data_test_scaled = min_max_scaler.fit_transform(df_test_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train_scaled).to_csv(\"./Results/dat_train_scaled.csv\", index = False)\n",
    "pd.DataFrame(data_test_scaled).to_csv(\"./Results/dat_test_scaled.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If want to run from here:\n",
    "# data_train_scaled = pd.read_csv(\"./Results/dat_train_scaled.csv\")\n",
    "# data_test_scaled = pd.read_csv(\"./Results/dat_test_scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with support vector machine classification and use probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "for i in range(1, 12):\n",
    "    \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=40)\n",
    "    train_features = pd.DataFrame(transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i]))\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = pd.DataFrame(transformer.transform(data_test_scaled))\n",
    "\n",
    "#     values_1 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 1]\n",
    "#     values_0 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 0]\n",
    "#     values_0 = resample(values_0, replace = False, n_samples = values_1.shape[0])\n",
    "\n",
    "#     train_features = pd.concat([values_0, values_1])\n",
    "    \n",
    "#     labels = np.repeat([0,1], values_0.shape[0])\n",
    "    \n",
    "    #clf = BaggingClassifier(SVC(kernel = 'poly', degree = 5, class_weight = 'balanced', verbose = True, C = 10))\n",
    "    clf_w = SVC(kernel = 'rbf', class_weight = 'balanced', verbose = 2)\n",
    "    \n",
    "    parameters = {'C':(0.1, 1, 5, 10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 5,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 2,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "#     clf.fit(train_features, labels)\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_features)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_features)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "0.3930185199739351\n",
      "R2 for feature LABEL_RRate  :  0.4976545747941705\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.6006515759436446\n",
      "R2 for feature LABEL_ABPm  :  0.7476064735396788\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "0.29494456706841204\n",
      "R2 for feature LABEL_SpO2  :  0.41853524315860036\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.6343183776716758\n",
      "R2 for feature LABEL_Heartrate  :  0.7733076418881326\n"
     ]
    }
   ],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "for i in range(12, 16):\n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 80)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "    clf_w = SVR(kernel = 'rbf', cache_size = 6000)\n",
    "# #     clf_w = NuSVR(nu=0.5, kernel = 'linear')\n",
    "    parameters = {'C':(0.1, 1,10, 20)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with Support vector regression and then compute sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "\n",
    "# columns_1 = [test_pids]\n",
    "\n",
    "# for i in range(1,12):\n",
    "    \n",
    "#     clf = SVR(kernel = 'poly', degree = 3, max_iter = 10000)\n",
    "#     clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     pred = clf.predict(data_test_scaled)\n",
    "#     prob = np.empty(len(pred))\n",
    "#     for j in range(0, len(pred)):\n",
    "#         prob[j] = 1 / (1 + math.exp(-pred[j]))\n",
    "#     columns_1.append(prob)\n",
    "    \n",
    "#     pred_train = clf.predict(data_train_scaled)\n",
    "#     prob_train = np.empty(len(pred_train))\n",
    "#     for j in range(0, len(pred_train)):\n",
    "#         prob_train[j] = 1 / (1 + math.exp(-pred_train[j]))    \n",
    "#     tmp = roc_auc_score(y_score= prob_train, y_true= df_train_labels.iloc[:,i])\n",
    "#     print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #labels that have a real value\n",
    "\n",
    "# columns_2 = []\n",
    "\n",
    "# for i in range(12, 16):\n",
    "    \n",
    "#     # feature selection\n",
    "#     transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "#     train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(list(data_train_scaled)[transformer.get_support()])\n",
    "#     test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "\n",
    "#     clf_w = LinearSVR()\n",
    "#     parameters = {'C':(0.1,1,10,30,60,100)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 2,\n",
    "#                                        refit = True, scoring = 'r2', verbose = 1, n_jobs=6)\n",
    "#     clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "#     pred = clf.predict(test_features)\n",
    "#     columns_2.append(pred)\n",
    "    \n",
    "#     pred_train = clf.predict(train_features)\n",
    "#     tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "#     print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,11])\n",
    "test_features = transformer.transform(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_agg_features.columns[transformer.get_support(indices = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Classifier\n",
    "columns_1 = [test_pids]\n",
    "for i in range(1, 12):\n",
    "    clf = RandomForestClassifier(min_samples_leaf=2, class_weight='balanced', oob_score=False, bootstrap=False)\n",
    "    clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(clf.oob_score)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    probability = clf.apply(data_test_scaled)\n",
    "    probs = [i[1] for i in probability] \n",
    "    columns_1.append(probs)\n",
    "    \n",
    "    \n",
    "    probability = clf.predict_proba(data_train_scaled)\n",
    "\n",
    "    probs = [i[1] for i in probability]            \n",
    "    tmp = roc_auc_score(y_score= probs, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernelized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   38.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79106093 0.82416662 0.83562059 0.83791284 0.83913964]\n",
      "[0.78686229 0.81519307 0.82233705 0.82308116 0.82318741]\n",
      "{'C': 20}\n",
      "0.8231874061865032\n",
      "ROC AUC for feature LABEL_BaseExcess  :  0.8383958848444988\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   35.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73157845 0.76224166 0.77745444 0.78103818 0.78303012]\n",
      "[0.71362649 0.72297582 0.72237351 0.72088357 0.71902811]\n",
      "{'C': 1}\n",
      "0.7229758203174029\n",
      "ROC AUC for feature LABEL_Fibrinogen  :  0.7609500263873665\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   35.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67183829 0.69027972 0.698827   0.70029654 0.70094057]\n",
      "[0.66352591 0.67026969 0.66975175 0.66847791 0.66717872]\n",
      "{'C': 1}\n",
      "0.6702696942419808\n",
      "ROC AUC for feature LABEL_AST  :  0.6897391342058248\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67570279 0.69259691 0.70046899 0.70185363 0.70244669]\n",
      "[0.66777187 0.67375573 0.67286269 0.67149621 0.67018327]\n",
      "{'C': 1}\n",
      "0.6737557253555625\n",
      "ROC AUC for feature LABEL_Alkalinephos  :  0.6920624444025381\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   35.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67600817 0.69334543 0.70092296 0.70222058 0.70279574]\n",
      "[0.66800198 0.67377846 0.67210889 0.67045184 0.66908811]\n",
      "{'C': 1}\n",
      "0.6737784601649827\n",
      "ROC AUC for feature LABEL_Bilirubin_total  :  0.6927177849663808\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   32.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.727936   0.74864355 0.75806401 0.75977426 0.76055451]\n",
      "[0.72104592 0.73110985 0.73177728 0.7305907  0.72931995]\n",
      "{'C': 5}\n",
      "0.7317772777005667\n",
      "ROC AUC for feature LABEL_Lactate  :  0.7568562664352593\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   41.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69043253 0.73212456 0.74886148 0.75229502 0.75405013]\n",
      "[0.67399859 0.69929012 0.70483802 0.70487099 0.70439942]\n",
      "{'C': 10}\n",
      "0.7048709867586446\n",
      "ROC AUC for feature LABEL_TroponinI  :  0.7500745884059313\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   35.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74921975 0.77084795 0.78108658 0.78311217 0.78415104]\n",
      "[0.74385934 0.75723627 0.76134574 0.76137675 0.76096035]\n",
      "{'C': 10}\n",
      "0.7613767544653565\n",
      "ROC AUC for feature LABEL_SaO2  :  0.7820885768495921\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   34.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72923182 0.7686555  0.78968114 0.79436461 0.79682402]\n",
      "[0.69669039 0.69346991 0.68202758 0.67602725 0.67048032]\n",
      "{'C': 0.1}\n",
      "0.6966903875607393\n",
      "ROC AUC for feature LABEL_Bilirubin_direct  :  0.7285286803806112\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   40.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80688026 0.83800213 0.85483244 0.85877412 0.86100069]\n",
      "[0.79515188 0.81093374 0.81642742 0.81664731 0.8158565 ]\n",
      "{'C': 10}\n",
      "0.8166473134626372\n",
      "ROC AUC for feature LABEL_EtCO2  :  0.8569790806165661\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:   36.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6911258  0.72333708 0.73863562 0.74194968 0.74373438]\n",
      "[0.66427809 0.66641978 0.65871854 0.65452763 0.6510455 ]\n",
      "{'C': 1}\n",
      "0.6664197755031103\n",
      "ROC AUC for feature LABEL_Sepsis  :  0.7211852229164408\n"
     ]
    }
   ],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "for i in range(1, 12):\n",
    "    \n",
    "    #     feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param = 80)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "   \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    \n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = LogisticRegression(penalty = 'l2', class_weight = 'balanced', max_iter=10000)\n",
    "    \n",
    "    # checked before\n",
    "    #parameters = {'alpha':(0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 30)}\n",
    "    parameters = {'C':(0.1, 1, 5, 10, 20)}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1)\n",
    "    \n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = skf,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 1,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_transformed, df_train_labels.iloc[:,i])\n",
    "    \n",
    "    print(clf.cv_results_['mean_train_score'])\n",
    "    print(clf.cv_results_['mean_test_score'])\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    " \n",
    "    probability_tmp = clf.predict_proba(test_transformed)\n",
    "    probability = [item[1] for item in probability_tmp]\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    probability_tmp = clf.predict_proba(train_transformed)\n",
    "    probability_train = [item[1] for item in probability_tmp]\n",
    "    tmp = roc_auc_score(y_score= probability_train, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the kernel and use SGD Classifier and Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "for i in range(1, 12):\n",
    "   \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=70)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'poly', degree = 3,\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDClassifier(max_iter=100000, tol=1e-4, penalty = \"l2\", \n",
    "                                       loss = \"epsilon_insensitive\", class_weight='balanced')\n",
    "    # checked before\n",
    "    #parameters = {'alpha':(0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 30)}\n",
    "    parameters = {'alpha':(0.1, 1, 5, 10)}\n",
    "    \n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 1,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_transformed, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_transformed)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_transformed)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=6)]: Done  70 out of  70 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.35232507  0.33166227  0.19226571  0.025843   -0.00711246 -0.01038657\n",
      " -0.01157302]\n",
      "[ 0.3489826   0.32950042  0.19135734  0.02531185 -0.0077462  -0.01107278\n",
      " -0.01220337]\n",
      "{'alpha': 0.0001}\n",
      "0.3489825991874099\n",
      "R2 for feature LABEL_RRate  :  0.3524597318842365\n",
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done  70 out of  70 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54154224  0.47640439  0.2441125   0.02493974 -0.0095532  -0.01286689\n",
      " -0.01325277]\n",
      "[ 0.5388466   0.47524918  0.24356913  0.02446396 -0.01002673 -0.01333567\n",
      " -0.0137103 ]\n",
      "{'alpha': 0.0001}\n",
      "0.5388466039811074\n",
      "R2 for feature LABEL_ABPm  :  0.542747157445501\n",
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=6)]: Done  70 out of  70 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25916808  0.2538305   0.18738713  0.03620747 -0.001805   -0.00474938\n",
      " -0.00549215]\n",
      "[ 0.26525103  0.2611547   0.1941885   0.03772106 -0.00194404 -0.00509215\n",
      " -0.00587499]\n",
      "{'alpha': 0.0001}\n",
      "0.2652510348586627\n",
      "R2 for feature LABEL_SpO2  :  0.2587643902397483\n",
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done  70 out of  70 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.82614703e-01  5.26892427e-01  1.97955428e-01  2.25251296e-02\n",
      "  2.80402635e-04 -1.78193368e-03 -2.01830352e-03]\n",
      "[ 5.80187794e-01  5.25814068e-01  1.97391032e-01  2.21304739e-02\n",
      " -9.00266875e-05 -2.15419455e-03 -2.39491718e-03]\n",
      "{'alpha': 0.0001}\n",
      "0.5801877939009681\n",
      "R2 for feature LABEL_Heartrate  :  0.5838083255008959\n"
     ]
    }
   ],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "\n",
    "for i in range(12, 16):\n",
    "#     feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 80)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_features = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_features = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDRegressor(max_iter=100000, tol=1e-4,\n",
    "                                     loss = 'epsilon_insensitive', penalty = 'l2')\n",
    "    parameters = {'alpha':(0.0001, 0.001, 0.01, 0.1, 1, 5, 10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                       refit = True, scoring = 'r2', verbose = 1, n_jobs=6, return_train_score=True)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.cv_results_['mean_train_score'])\n",
    "    print(clf.cv_results_['mean_test_score'])\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred) \n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of XGBoost is that it accepts missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# simple example\n",
    "# load file from text file, also binary buffer generated by xgboost\n",
    "\n",
    "# scale test data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "df_train_agg_features = min_max_scaler.fit_transform(df_train_agg_features)\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "df_test_agg_features = min_max_scaler.fit_transform(df_test_agg_features)\n",
    "\n",
    "dtrain = xgb.DMatrix(df_train_agg_features, label=df_train_labels.iloc[:,11])\n",
    "dtest = xgb.DMatrix(df_test_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 1, 'objective': 'binary:logistic', 'verbosity':1}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "evallist = [(dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "xgb.cv(param, dtrain, num_round, nfold=10,\n",
    "       metrics={'auc'}, seed=0,\n",
    "       callbacks=[xgb.callback.print_evaluation(show_stdv=True)])\n",
    "# bst = xgb.train(param, dtrain, num_round)\n",
    "# pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "\n",
    "for i in range(1,12):\n",
    "\n",
    "    clf = xgb.XGBClassifier('binary:logistic')\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1)\n",
    "    clf_w = model_selection.GridSearchCV(estimator= clf, param_grid = {'max_depth': [2,4,6,8,10]}, cv = skf,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 1,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    \n",
    "\n",
    "    clf_w.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     dtrain = xgb.DMatrix(df_train_agg_features, label=df_train_labels.iloc[:,i])\n",
    "#     num_round = 10\n",
    "#     xgb.cv(param, dtrain, num_round, nfold=10,\n",
    "#        metrics={'auc'}, seed=0,\n",
    "#        callbacks=[xgb.callback.print_evaluation(show_stdv=True)])\n",
    "    \n",
    "    probability = clf_w.predict(test_features)\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    probability_train = clf_w.predict(train_features)\n",
    "#     probability_train = [item[1] for item in probability_tmp]\n",
    "    tmp = roc_auc_score(y_score= probability_train, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2 = []\n",
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'reg:squarederror', 'verbosity':1}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'rmse'\n",
    "evallist = [(dtrain, 'train')]\n",
    "\n",
    "for i in range(12,16):\n",
    "\n",
    "    clf_w = xgb.XGBRegressor(feval = r2_score)\n",
    "        \n",
    "    clf_w.fit(df_train_agg_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "    dtrain = xgb.DMatrix(df_train_agg_features, label=df_train_labels.iloc[:,i])\n",
    "    num_round = 10\n",
    "    xgb.cv(param, dtrain, num_round, nfold=10, seed=0,\n",
    "       callbacks=[xgb.callback.print_evaluation(show_stdv=True)])\n",
    "    \n",
    "    probability = clf_w.predict(df_test_agg_features)\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    pred_train = clf_w.predict(df_train_agg_features)\n",
    "#     probability_train = [item[1] for item in probability_tmp]\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 12664)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(columns_final))\n",
    "result = pd.DataFrame(columns_final).transpose()\n",
    "result.columns = list(df_train_labels)\n",
    "result.to_csv('./Results/prediction.csv.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Results/prediction.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
