{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, mutual_info_classif, mutual_info_regression, f_regression\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "# load data from csv file\n",
    "df_train_features = pd.read_csv ('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Load test data\n",
    "df_test_features = pd.read_csv ('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels = df_train_labels.sort_values(by = 'pid')\n",
    "df_train_features = df_train_features.sort_values(by = 'pid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Histogram of the output labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.hist()\n",
    "\n",
    "# with PdfPages(\"./Results/Labels_histogram.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see the class imbalance problem here. Other observations:\n",
    "  * Heartrate, RRate, ABPm,  distribution is similar to a normal distribution\n",
    "  * SpO2 is like a censored normal distribution. \n",
    "  * For all of the other features, class imbalance is an obvious problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic strategy that could be used here: Upsample both classes! Do the upsampling efficiently, not just replicating the datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inspection: \n",
    "#############################################\n",
    "# range of the provided data?\n",
    "print(df_train_features.agg([min, max]))\n",
    "\n",
    "# Boxplotting the data\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.set_title('BUN')\n",
    "# ax2.boxplot(df_train_features.iloc[:,5], notch=True)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = df_train_features.iloc[:,1:])\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "# with PdfPages(\"./Results/Train_columns_boxplot.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = df_train_features.corr()\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, \n",
    "        vmin=-1, vmax=1, center=0, \n",
    "           cmap=sns.diverging_palette(20, 220, n=200))\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing pattern of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much missing data? \n",
    "print(\"Percentage of missing values:\")\n",
    "print(df_train_features.isnull().sum(axis=0) / len(df_train_features))\n",
    "\n",
    "msno.matrix(df_train_features)\n",
    "\n",
    "# Plotting the correlation between the missing values\n",
    "msno.heatmap(df_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(            'Age', 'mean'),\n",
      "            (          'EtCO2', 'amin'),\n",
      "            (            'PTT', 'amin'),\n",
      "            (            'PTT', 'amax'),\n",
      "            (            'PTT', 'mean'),\n",
      "            (            'BUN', 'amin'),\n",
      "            (            'BUN', 'amax'),\n",
      "            (            'BUN', 'mean'),\n",
      "            (        'Lactate', 'amin'),\n",
      "            (        'Lactate', 'amax'),\n",
      "            ...\n",
      "            ('Bilirubin_total', 'mean'),\n",
      "            (      'TroponinI', 'amin'),\n",
      "            (      'TroponinI', 'amax'),\n",
      "            (      'TroponinI', 'mean'),\n",
      "            (           'ABPs', 'amin'),\n",
      "            (           'ABPs', 'amax'),\n",
      "            (           'ABPs', 'mean'),\n",
      "            (             'pH', 'amin'),\n",
      "            (             'pH', 'amax'),\n",
      "            (             'pH', 'mean')],\n",
      "           length=101)\n",
      "101\n",
      "33\n",
      "number of missing values:\n",
      "Age    mean        0\n",
      "EtCO2  amin    17673\n",
      "PTT    amin    11179\n",
      "       amax    11179\n",
      "       mean    11179\n",
      "               ...  \n",
      "ABPs   amax      398\n",
      "       mean      398\n",
      "pH     amin    11001\n",
      "       amax    11001\n",
      "       mean    11001\n",
      "Length: 101, dtype: int64\n",
      "('EtCO2', 'amin')\n",
      "17673\n",
      "should be removed\n",
      "('PTT', 'amin')\n",
      "11179\n",
      "('PTT', 'amax')\n",
      "11179\n",
      "('PTT', 'mean')\n",
      "11179\n",
      "('BUN', 'amin')\n",
      "5029\n",
      "('BUN', 'amax')\n",
      "5029\n",
      "('BUN', 'mean')\n",
      "5029\n",
      "('Lactate', 'amin')\n",
      "14123\n",
      "('Lactate', 'amax')\n",
      "14123\n",
      "('Lactate', 'mean')\n",
      "14123\n",
      "('Temp', 'amin')\n",
      "443\n",
      "('Temp', 'amax')\n",
      "443\n",
      "('Temp', 'mean')\n",
      "443\n",
      "('Hgb', 'amin')\n",
      "5020\n",
      "('Hgb', 'amax')\n",
      "5020\n",
      "('Hgb', 'mean')\n",
      "5020\n",
      "('HCO3', 'amin')\n",
      "11158\n",
      "('HCO3', 'amax')\n",
      "11158\n",
      "('HCO3', 'mean')\n",
      "11158\n",
      "('BaseExcess', 'amin')\n",
      "13298\n",
      "('BaseExcess', 'amax')\n",
      "13298\n",
      "('BaseExcess', 'mean')\n",
      "13298\n",
      "('RRate', 'amin')\n",
      "152\n",
      "('RRate', 'amax')\n",
      "152\n",
      "('RRate', 'mean')\n",
      "152\n",
      "('Fibrinogen', 'amin')\n",
      "17281\n",
      "should be removed\n",
      "('Fibrinogen', 'amax')\n",
      "17281\n",
      "should be removed\n",
      "('Fibrinogen', 'mean')\n",
      "17281\n",
      "should be removed\n",
      "('Phosphate', 'amin')\n",
      "9666\n",
      "('Phosphate', 'amax')\n",
      "9666\n",
      "('Phosphate', 'mean')\n",
      "9666\n",
      "('WBC', 'amin')\n",
      "5654\n"
     ]
    }
   ],
   "source": [
    "df_train_agg_features = df_train_features.groupby('pid').agg([np.min, np.max, np.mean])\n",
    "df_train_agg_features = df_train_agg_features.iloc[:,5:]\n",
    "# Removing ETCo2 mean and max since it has so many NA\n",
    "df_train_agg_features = df_train_agg_features.drop(df_train_agg_features.columns[[2,3]],  axis = 1)\n",
    "print(df_train_agg_features.columns)\n",
    "df_train_agg_features.columns\n",
    "print(int(df_train_agg_features.shape[1]))\n",
    "print(int(df_train_agg_features.shape[1]/3))\n",
    "\n",
    "# how much missing data? \n",
    "print(\"number of missing values:\")\n",
    "print(df_train_agg_features.isnull().sum(axis=0))\n",
    "\n",
    "na_percent_max = int(0.8 * df_train_agg_features.shape[0])\n",
    "tmp = pd.DataFrame(df_train_agg_features)\n",
    "for i in range(1, (int(df_train_agg_features.shape[1]/3))):\n",
    "    na_count = df_train_agg_features.iloc[:,i].isna().sum()\n",
    "    print(df_train_agg_features.columns[i])\n",
    "    print(na_count)\n",
    "    \n",
    "    if(na_count > na_percent_max):\n",
    "        print(\"should be removed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "#imp = SimpleImputer(strategy=\"mean\")\n",
    "imputer = KNNImputer(n_neighbors = 10)\n",
    "#imputer = IterativeImputer(random_state=0, verbose = 2, max_iter = 30)\n",
    "df_train_agg_imputed_features = imputer.fit_transform(df_train_agg_features)\n",
    "#print(df_train_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "# standard_scalar = preprocessing.StandardScaler()\n",
    "\n",
    "data_train_scaled = min_max_scaler.fit_transform(df_train_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REARRANGE THE LABELS, TO MATCH THE REARRANGED FEATURES\n",
    "df_train_labels_sorted = df_train_labels.sort_values(by = 'pid')\n",
    "print(df_train_labels_sorted[['pid']])\n",
    "print(df_train_labels[['pid']])\n",
    "print(df_train_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training data after imputing and aggregating\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = pd.DataFrame(data_train_scaled))\n",
    "ax.set_xticklabels(\n",
    "    list(df_train_features),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the correlation between the \n",
    "pd.DataFrame(data_train_scaled).corrwith(other = pd.DataFrame(df_train_agg_imputed_features), method = \"spearman\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(data_train_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, df_train_labels[[df_train_labels.columns[11]]]], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA for label i', fontsize = 20)\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[df_train_labels.columns[11]] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inspection: \n",
    "#############################################\n",
    "# range of the provided data?\n",
    "print(df_test_features.agg([min, max]))\n",
    "\n",
    "# how much missing data? \n",
    "print(\"number of missing values:\")\n",
    "print(df_test_features.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate data for each pid\n",
    "# df_test_aggregate_features = df_test_features.groupby('pid').agg('median')\n",
    "\n",
    "df_test_agg_features = df_test_features.groupby('pid').agg([np.min, np.max, np.mean])\n",
    "\n",
    "df_test_agg_features = df_test_agg_features.iloc[:,5:]\n",
    "# Removing ETCo2 mean and max since it has so many NA\n",
    "df_test_agg_features = df_test_agg_features.drop(df_test_agg_features.columns[[2,3]],  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "# should we impute it with the same imputer that we've used for train?\n",
    "\n",
    "imputer = KNNImputer(n_neighbors= 10)\n",
    "#imputer = IterativeImputer(random_state=0, verbose = 1)\n",
    "df_test_agg_imputed_features = imputer.fit_transform(df_test_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "data_test_scaled = min_max_scaler.fit_transform(df_test_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data_train_scaled).to_csv(\"./Results/4stats_iterarive_dat_train_scaled.csv\")\n",
    "# pd.DataFrame(data_test_scaled).to_csv(\"./Results/4stats_iterative_dat_test_scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with support vector machine classification and use probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature  LABEL_BaseExcess\n",
      "MultiIndex([(       'PTT', 'amin'),\n",
      "            (       'PTT', 'amax'),\n",
      "            (       'PTT', 'mean'),\n",
      "            (   'Lactate', 'amin'),\n",
      "            (   'Lactate', 'amax'),\n",
      "            (   'Lactate', 'mean'),\n",
      "            (      'Temp', 'mean'),\n",
      "            (       'Hgb', 'mean'),\n",
      "            (      'HCO3', 'amin'),\n",
      "            (      'HCO3', 'amax'),\n",
      "            (      'HCO3', 'mean'),\n",
      "            ('BaseExcess', 'amin'),\n",
      "            ('BaseExcess', 'amax'),\n",
      "            ('BaseExcess', 'mean'),\n",
      "            (     'RRate', 'mean'),\n",
      "            ('Creatinine', 'amin'),\n",
      "            ('Creatinine', 'amax'),\n",
      "            ('Creatinine', 'mean'),\n",
      "            (     'PaCO2', 'amin'),\n",
      "            (     'PaCO2', 'amax'),\n",
      "            (     'PaCO2', 'mean'),\n",
      "            (      'FiO2', 'amin'),\n",
      "            (      'FiO2', 'amax'),\n",
      "            (      'FiO2', 'mean'),\n",
      "            (      'SaO2', 'amin'),\n",
      "            (      'SaO2', 'amax'),\n",
      "            (      'SaO2', 'mean'),\n",
      "            (      'ABPm', 'mean'),\n",
      "            (      'ABPd', 'mean'),\n",
      "            (   'Calcium', 'amin'),\n",
      "            (   'Calcium', 'amax'),\n",
      "            (   'Calcium', 'mean'),\n",
      "            (  'Chloride', 'amin'),\n",
      "            (  'Chloride', 'amax'),\n",
      "            (  'Chloride', 'mean'),\n",
      "            (       'Hct', 'amax'),\n",
      "            (      'ABPs', 'mean'),\n",
      "            (        'pH', 'amin'),\n",
      "            (        'pH', 'amax'),\n",
      "            (        'pH', 'mean')],\n",
      "           )\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 out of  20 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-87eea5060d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                         \u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                        n_jobs=6, return_train_score = True)\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "for i in range(1, 12):\n",
    "   \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=40)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "    print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "\n",
    "    \n",
    "    #clf = BaggingClassifier(SVC(kernel = 'poly', degree = 5, class_weight = 'balanced', verbose = True, C = 10))\n",
    "    clf_w = SVC(kernel = 'poly', degree = 3, class_weight = 'balanced', verbose = 2)\n",
    "    \n",
    "    parameters = {'C':(0.1, 1, 5, 10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 5,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 2,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "    print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_features)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_features)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([22.40885172, 22.84809146, 21.25310478]), 'std_fit_time': array([0.06816745, 0.13452873, 3.31278865]), 'mean_score_time': array([4.55382714, 4.49709716, 3.69804029]), 'std_score_time': array([0.01893634, 0.04048154, 0.52262203]), 'param_C': masked_array(data=[0.1, 1, 10],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.1}, {'C': 1}, {'C': 10}], 'split0_test_score': array([0.52470667, 0.57949584, 0.60073112]), 'split1_test_score': array([0.51697736, 0.57453686, 0.60189326]), 'split2_test_score': array([0.53313793, 0.58945065, 0.60719138]), 'split3_test_score': array([0.52378756, 0.58424314, 0.61198652]), 'split4_test_score': array([0.51733216, 0.56972221, 0.58543835]), 'mean_test_score': array([0.52318833, 0.57948974, 0.60144813]), 'std_test_score': array([0.00590813, 0.00695342, 0.0089562 ]), 'rank_test_score': array([3, 2, 1], dtype=int32)}\n",
      "{'C': 10}\n",
      "0.6014481277020215\n",
      "R2 for feature LABEL_Heartrate  :  0.6959475560817752\n"
     ]
    }
   ],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "for i in range(12, 16):\n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 50)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "    clf_w = SVR(kernel = 'rbf', gamma = 'scale', cache_size = 6000)\n",
    "# #     clf_w = NuSVR(nu=0.5, kernel = 'linear')\n",
    "    parameters = {'C':(0.1, 1,10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 5,\n",
    "                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "    print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with Support vector regression and then compute sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "\n",
    "# columns_1 = [test_pids]\n",
    "\n",
    "# for i in range(1,12):\n",
    "    \n",
    "#     clf = SVR(kernel = 'poly', degree = 3, max_iter = 10000)\n",
    "#     clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     pred = clf.predict(data_test_scaled)\n",
    "#     prob = np.empty(len(pred))\n",
    "#     for j in range(0, len(pred)):\n",
    "#         prob[j] = 1 / (1 + math.exp(-pred[j]))\n",
    "#     columns_1.append(prob)\n",
    "    \n",
    "#     pred_train = clf.predict(data_train_scaled)\n",
    "#     prob_train = np.empty(len(pred_train))\n",
    "#     for j in range(0, len(pred_train)):\n",
    "#         prob_train[j] = 1 / (1 + math.exp(-pred_train[j]))    \n",
    "#     tmp = roc_auc_score(y_score= prob_train, y_true= df_train_labels.iloc[:,i])\n",
    "#     print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #labels that have a real value\n",
    "\n",
    "# columns_2 = []\n",
    "\n",
    "# for i in range(12, 16):\n",
    "    \n",
    "#     # feature selection\n",
    "#     transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "#     train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(list(data_train_scaled)[transformer.get_support()])\n",
    "#     test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "\n",
    "#     clf_w = LinearSVR()\n",
    "#     parameters = {'C':(0.1,1,10,30,60,100)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 2,\n",
    "#                                        refit = True, scoring = 'r2', verbose = 1, n_jobs=6)\n",
    "#     clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "#     pred = clf.predict(test_features)\n",
    "#     columns_2.append(pred)\n",
    "    \n",
    "#     pred_train = clf.predict(train_features)\n",
    "#     tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "#     print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,11])\n",
    "test_features = transformer.transform(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_agg_features.columns[transformer.get_support(indices = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Classifier\n",
    "columns_1 = [test_pids]\n",
    "for i in range(1, 12):\n",
    "    clf = RandomForestClassifier(min_samples_leaf=2, class_weight='balanced', oob_score=False, bootstrap=False)\n",
    "    clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(clf.oob_score)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    probability = clf.apply(data_test_scaled)\n",
    "    probs = [i[1] for i in probability] \n",
    "    columns_1.append(probs)\n",
    "    \n",
    "    \n",
    "    probability = clf.predict_proba(data_train_scaled)\n",
    "\n",
    "    probs = [i[1] for i in probability]            \n",
    "    tmp = roc_auc_score(y_score= probs, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the kernel and use SGD Classifier and Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature  LABEL_BaseExcess\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  50 out of  50 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "0.73635919374533\n",
      "ROC AUC for feature LABEL_BaseExcess  :  0.7380687377642452\n",
      "For feature  LABEL_Fibrinogen\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn import linear_model\n",
    "\n",
    "for i in range(1, 12):\n",
    "   \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=80)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDClassifier(max_iter=100000, tol=1e-4, penalty = \"l2\", \n",
    "                                       loss = \"epsilon_insensitive\", class_weight='balanced')\n",
    "    \n",
    "    parameters = {'alpha':(1, 5, 10, 20, 30)}\n",
    "    \n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 1,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_features)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_features)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.37372692090202514\n",
      "R2 for feature LABEL_RRate  :  0.3767403190964719\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.5625391293347101\n",
      "R2 for feature LABEL_ABPm  :  0.5655713152749196\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.3627442534699542\n",
      "R2 for feature LABEL_SpO2  :  0.37501285055117395\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.5621301547339839\n",
      "R2 for feature LABEL_Heartrate  :  0.5637619374402578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "\n",
    "for i in range(12, 16):\n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 80)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDRegressor(max_iter=100000, tol=1e-4,\n",
    "                                     loss = 'epsilon_insensitive', penalty = 'l2')\n",
    "    parameters = {'alpha':(0.1, 1, 5, 10, 20, 30)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                       refit = True, scoring = 'r2', verbose = 1, n_jobs=6)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 12664)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(columns_final))\n",
    "result = pd.DataFrame(columns_final).transpose()\n",
    "result.columns = list(df_train_labels)\n",
    "result.to_csv('./Results/prediction.csv.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Results/prediction.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
