{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, mutual_info_classif, mutual_info_regression, f_regression\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "\n",
    "# load data from csv file\n",
    "df_train_features = pd.read_csv ('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Load test data\n",
    "df_test_features = pd.read_csv ('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels = df_train_labels.sort_values(by = 'pid')\n",
    "df_train_features = df_train_features.sort_values(by = 'pid')\n",
    "\n",
    "# Droping time\n",
    "df_train_features = df_train_features.drop('Time', axis = 1)\n",
    "df_test_features = df_test_features.drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Histogram of the output labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.hist()\n",
    "\n",
    "# with PdfPages(\"./Results/Labels_histogram.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see the class imbalance problem here. Other observations:\n",
    "  * Heartrate, RRate, ABPm,  distribution is similar to a normal distribution\n",
    "  * SpO2 is like a censored normal distribution. \n",
    "  * For all of the other features, class imbalance is an obvious problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic strategy that could be used here: Upsample both classes! Do the upsampling efficiently, not just replicating the datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot over features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inspection: \n",
    "#############################################\n",
    "# range of the provided data?\n",
    "print(df_train_features.agg([min, max]))\n",
    "\n",
    "# Boxplotting the data\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.set_title('BUN')\n",
    "# ax2.boxplot(df_train_features.iloc[:,5], notch=True)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = df_train_features.iloc[:,1:])\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "# with PdfPages(\"./Results/Train_columns_boxplot.pdf\") as export_pdf:\n",
    "#     for i in list(df_train_labels)[1:]:\n",
    "#         df_train_labels.hist(column = i, bins = 100)\n",
    "#         export_pdf.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = df_train_features.corr()\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, \n",
    "        vmin=-1, vmax=1, center=0, \n",
    "           cmap=sns.diverging_palette(20, 220, n=200))\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing pattern of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much missing data? \n",
    "print(\"Percentage of missing values:\")\n",
    "print(df_train_features.isnull().sum(axis=0) / len(df_train_features))\n",
    "\n",
    "msno.matrix(df_train_features)\n",
    "\n",
    "# Plotting the correlation between the missing values\n",
    "msno.heatmap(df_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  6 10 20 21 24 27 31 34]\n",
      "[ 7 15 17 23 30 35]\n",
      "[ 2  3  4  5  8  9 11 12 13 14 16 18 19 22 25 26 28 29 32 33]\n",
      "(18995, 41)\n",
      "(18995, 18)\n",
      "(18995, 20)\n"
     ]
    }
   ],
   "source": [
    "# Which columns have less than a percent NA\n",
    "NA_percent = 0.8\n",
    "NA_percent_severe = 0.91\n",
    "\n",
    "sel_features = df_train_features.isnull().sum(axis=0) < (NA_percent * df_train_features.shape[0])\n",
    "inds = np.where(sel_features == True)\n",
    "\n",
    "sel_features_2 = (df_train_features.isnull().sum(axis=0) < (NA_percent_severe * df_train_features.shape[0])) & (df_train_features.isnull().sum(axis=0) > (NA_percent * df_train_features.shape[0]))        \n",
    "inds_2 = np.where(sel_features_2 == True)\n",
    "\n",
    "sel_features_3 = df_train_features.isnull().sum(axis=0) > (NA_percent_severe * df_train_features.shape[0])\n",
    "inds_3 = np.where(sel_features_3 == True)\n",
    "\n",
    "print(inds[0])\n",
    "print(inds_2[0])\n",
    "print(inds_3[0])\n",
    "\n",
    "present_columns = df_train_features.iloc[:,inds[0]]\n",
    "\n",
    "present_columns_agg = present_columns.groupby('pid').agg([np.min, np.max, np.mean, np.std, lambda x: x.median() - x.mean()])\n",
    "present_columns_agg = present_columns_agg.drop(present_columns_agg.columns[[0,1,3,4]], axis=1)\n",
    "\n",
    "missing_columns = df_train_features.iloc[:,np.append(0,inds_2)]\n",
    "\n",
    "missing_columns_agg = missing_columns.groupby('pid').agg([np.min, np.max, np.mean])\n",
    "\n",
    "missing_columns_severe = df_train_features.iloc[:,np.append(0,inds_3)]\n",
    "\n",
    "missing_columns_agg_severe = missing_columns_severe.groupby('pid').agg(np.mean)\n",
    "\n",
    "\n",
    "print(present_columns_agg.shape)\n",
    "print(missing_columns_agg.shape)\n",
    "print(missing_columns_agg_severe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 79)\n",
      "Index([            ('Age', 'mean'),            ('Temp', 'amin'),\n",
      "                  ('Temp', 'amax'),            ('Temp', 'mean'),\n",
      "                   ('Temp', 'std'),      ('Temp', '<lambda_0>'),\n",
      "                 ('RRate', 'amin'),           ('RRate', 'amax'),\n",
      "                 ('RRate', 'mean'),            ('RRate', 'std'),\n",
      "           ('RRate', '<lambda_0>'),         ('Glucose', 'amin'),\n",
      "               ('Glucose', 'amax'),         ('Glucose', 'mean'),\n",
      "                ('Glucose', 'std'),   ('Glucose', '<lambda_0>'),\n",
      "                  ('ABPm', 'amin'),            ('ABPm', 'amax'),\n",
      "                  ('ABPm', 'mean'),             ('ABPm', 'std'),\n",
      "            ('ABPm', '<lambda_0>'),            ('ABPd', 'amin'),\n",
      "                  ('ABPd', 'amax'),            ('ABPd', 'mean'),\n",
      "                   ('ABPd', 'std'),      ('ABPd', '<lambda_0>'),\n",
      "                  ('SpO2', 'amin'),            ('SpO2', 'amax'),\n",
      "                  ('SpO2', 'mean'),             ('SpO2', 'std'),\n",
      "            ('SpO2', '<lambda_0>'),       ('Heartrate', 'amin'),\n",
      "             ('Heartrate', 'amax'),       ('Heartrate', 'mean'),\n",
      "              ('Heartrate', 'std'), ('Heartrate', '<lambda_0>'),\n",
      "                  ('ABPs', 'amin'),            ('ABPs', 'amax'),\n",
      "                  ('ABPs', 'mean'),             ('ABPs', 'std'),\n",
      "            ('ABPs', '<lambda_0>'),             ('Hgb', 'amin'),\n",
      "                   ('Hgb', 'amax'),             ('Hgb', 'mean'),\n",
      "                 ('PaCO2', 'amin'),           ('PaCO2', 'amax'),\n",
      "                 ('PaCO2', 'mean'),            ('FiO2', 'amin'),\n",
      "                  ('FiO2', 'amax'),            ('FiO2', 'mean'),\n",
      "             ('Potassium', 'amin'),       ('Potassium', 'amax'),\n",
      "             ('Potassium', 'mean'),             ('Hct', 'amin'),\n",
      "                   ('Hct', 'amax'),             ('Hct', 'mean'),\n",
      "                    ('pH', 'amin'),              ('pH', 'amax'),\n",
      "                    ('pH', 'mean'),                     'EtCO2',\n",
      "                             'PTT',                       'BUN',\n",
      "                         'Lactate',                      'HCO3',\n",
      "                      'BaseExcess',                'Fibrinogen',\n",
      "                       'Phosphate',                       'WBC',\n",
      "                      'Creatinine',                       'AST',\n",
      "                       'Platelets',                      'SaO2',\n",
      "                       'Magnesium',                   'Calcium',\n",
      "                    'Alkalinephos',          'Bilirubin_direct',\n",
      "                        'Chloride',           'Bilirubin_total',\n",
      "                       'TroponinI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train_agg_features = pd.merge(present_columns_agg, missing_columns_agg, on=\"pid\")\n",
    "df_train_agg_features = pd.merge(df_train_agg_features, missing_columns_agg_severe, on = \"pid\")\n",
    "print(df_train_agg_features.shape)\n",
    "print(df_train_agg_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_agg_features = df_train_features.groupby('pid').agg([np.min, np.max, np.mean np.std])\n",
    "# df_train_agg_features = df_train_agg_features.iloc[:,5:]\n",
    "# # Removing ETCo2 mean and max since it has so many NA\n",
    "# df_train_agg_features = df_train_agg_features.drop(df_train_agg_features.columns[[2,3]],  axis = 1)\n",
    "# print(df_train_agg_features.columns)\n",
    "# df_train_agg_features.columns\n",
    "# print(int(df_train_agg_features.shape[1]))\n",
    "# print(int(df_train_agg_features.shape[1]/3))\n",
    "\n",
    "# # how much missing data? \n",
    "# print(\"number of missing values:\")\n",
    "# print(df_train_agg_features.isnull().sum(axis=0))\n",
    "\n",
    "# na_percent_max = int(0.8 * df_train_agg_features.shape[0])\n",
    "# tmp = pd.DataFrame(df_train_agg_features)\n",
    "# for i in range(1, (int(df_train_agg_features.shape[1]/3))):\n",
    "#     na_count = df_train_agg_features.iloc[:,i].isna().sum()\n",
    "#     print(df_train_agg_features.columns[i])\n",
    "#     print(na_count)\n",
    "    \n",
    "#     if(na_count > na_percent_max):\n",
    "#         print(\"should be removed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "#imp = SimpleImputer(strategy=\"mean\")\n",
    "imputer = KNNImputer(n_neighbors = 10)\n",
    "#imputer = IterativeImputer(random_state=0, verbose = 2, max_iter = 30)\n",
    "df_train_agg_imputed_features = imputer.fit_transform(df_train_agg_features)\n",
    "#print(df_train_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "# standard_scalar = preprocessing.StandardScaler()\n",
    "\n",
    "data_train_scaled = min_max_scaler.fit_transform(df_train_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REARRANGE THE LABELS, TO MATCH THE REARRANGED FEATURES\n",
    "df_train_labels_sorted = df_train_labels.sort_values(by = 'pid')\n",
    "print(df_train_labels_sorted[['pid']])\n",
    "print(df_train_labels[['pid']])\n",
    "print(df_train_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the training data after imputing and aggregating\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.boxplot(data = pd.DataFrame(data_train_scaled))\n",
    "ax.set_xticklabels(\n",
    "    list(df_train_features),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the correlation between the \n",
    "pd.DataFrame(data_train_scaled).corrwith(other = pd.DataFrame(df_train_agg_imputed_features), method = \"spearman\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(data_train_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, df_train_labels[[df_train_labels.columns[11]]]], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA for label i', fontsize = 20)\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[df_train_labels.columns[11]] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data inspection: \n",
    "#############################################\n",
    "# range of the provided data?\n",
    "print(df_test_features.agg([min, max]))\n",
    "\n",
    "# how much missing data? \n",
    "print(\"number of missing values:\")\n",
    "print(df_test_features.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  6 10 20 21 24 27 31 34]\n",
      "[ 7 15 17 23 30 35]\n",
      "[ 2  3  4  5  8  9 11 12 13 14 16 18 19 22 25 26 28 29 32 33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1113: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# We should use the same indices as before\n",
    "print(inds[0])\n",
    "print(inds_2[0])\n",
    "print(inds_3[0])\n",
    "\n",
    "present_columns = df_test_features.iloc[:,inds[0]]\n",
    "\n",
    "present_columns_agg = present_columns.groupby('pid').agg([np.min, np.max, np.mean, np.std, lambda x: x.median() - x.mean()])\n",
    "present_columns_agg = present_columns_agg.drop(present_columns_agg.columns[[0,1,3,4]], axis=1)\n",
    "\n",
    "missing_columns = df_test_features.iloc[:,np.append(0,inds_2)]\n",
    "\n",
    "missing_columns_agg = missing_columns.groupby('pid').agg([np.min, np.max, np.mean])\n",
    "\n",
    "missing_columns_severe = df_test_features.iloc[:,np.append(0,inds_3)]\n",
    "\n",
    "missing_columns_agg_severe = missing_columns_severe.groupby('pid').agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12664, 79)\n",
      "Index([            ('Age', 'mean'),            ('Temp', 'amin'),\n",
      "                  ('Temp', 'amax'),            ('Temp', 'mean'),\n",
      "                   ('Temp', 'std'),      ('Temp', '<lambda_0>'),\n",
      "                 ('RRate', 'amin'),           ('RRate', 'amax'),\n",
      "                 ('RRate', 'mean'),            ('RRate', 'std'),\n",
      "           ('RRate', '<lambda_0>'),         ('Glucose', 'amin'),\n",
      "               ('Glucose', 'amax'),         ('Glucose', 'mean'),\n",
      "                ('Glucose', 'std'),   ('Glucose', '<lambda_0>'),\n",
      "                  ('ABPm', 'amin'),            ('ABPm', 'amax'),\n",
      "                  ('ABPm', 'mean'),             ('ABPm', 'std'),\n",
      "            ('ABPm', '<lambda_0>'),            ('ABPd', 'amin'),\n",
      "                  ('ABPd', 'amax'),            ('ABPd', 'mean'),\n",
      "                   ('ABPd', 'std'),      ('ABPd', '<lambda_0>'),\n",
      "                  ('SpO2', 'amin'),            ('SpO2', 'amax'),\n",
      "                  ('SpO2', 'mean'),             ('SpO2', 'std'),\n",
      "            ('SpO2', '<lambda_0>'),       ('Heartrate', 'amin'),\n",
      "             ('Heartrate', 'amax'),       ('Heartrate', 'mean'),\n",
      "              ('Heartrate', 'std'), ('Heartrate', '<lambda_0>'),\n",
      "                  ('ABPs', 'amin'),            ('ABPs', 'amax'),\n",
      "                  ('ABPs', 'mean'),             ('ABPs', 'std'),\n",
      "            ('ABPs', '<lambda_0>'),             ('Hgb', 'amin'),\n",
      "                   ('Hgb', 'amax'),             ('Hgb', 'mean'),\n",
      "                 ('PaCO2', 'amin'),           ('PaCO2', 'amax'),\n",
      "                 ('PaCO2', 'mean'),            ('FiO2', 'amin'),\n",
      "                  ('FiO2', 'amax'),            ('FiO2', 'mean'),\n",
      "             ('Potassium', 'amin'),       ('Potassium', 'amax'),\n",
      "             ('Potassium', 'mean'),             ('Hct', 'amin'),\n",
      "                   ('Hct', 'amax'),             ('Hct', 'mean'),\n",
      "                    ('pH', 'amin'),              ('pH', 'amax'),\n",
      "                    ('pH', 'mean'),                     'EtCO2',\n",
      "                             'PTT',                       'BUN',\n",
      "                         'Lactate',                      'HCO3',\n",
      "                      'BaseExcess',                'Fibrinogen',\n",
      "                       'Phosphate',                       'WBC',\n",
      "                      'Creatinine',                       'AST',\n",
      "                       'Platelets',                      'SaO2',\n",
      "                       'Magnesium',                   'Calcium',\n",
      "                    'Alkalinephos',          'Bilirubin_direct',\n",
      "                        'Chloride',           'Bilirubin_total',\n",
      "                       'TroponinI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_test_agg_features = pd.merge(present_columns_agg, missing_columns_agg, on=\"pid\")\n",
    "df_test_agg_features = pd.merge(df_test_agg_features, missing_columns_agg_severe, on = \"pid\")\n",
    "print(df_test_agg_features.shape)\n",
    "print(df_test_agg_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # aggregate data for each pid\n",
    "# # df_test_aggregate_features = df_test_features.groupby('pid').agg('median')\n",
    "\n",
    "# df_test_agg_features = df_test_features.groupby('pid').agg([np.min, np.max, np.mean])\n",
    "\n",
    "# df_test_agg_features = df_test_agg_features.iloc[:,5:]\n",
    "# # Removing ETCo2 mean and max since it has so many NA\n",
    "# df_test_agg_features = df_test_agg_features.drop(df_test_agg_features.columns[[2,3]],  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data points\n",
    "# should we impute it with the same imputer that we've used for train?\n",
    "\n",
    "imputer = KNNImputer(n_neighbors= 10)\n",
    "#imputer = IterativeImputer(random_state=0, verbose = 1)\n",
    "df_test_agg_imputed_features = imputer.fit_transform(df_test_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "data_test_scaled = min_max_scaler.fit_transform(df_test_agg_imputed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data_train_scaled).to_csv(\"./Results/4stats_iterarive_dat_train_scaled.csv\")\n",
    "# pd.DataFrame(data_test_scaled).to_csv(\"./Results/4stats_iterative_dat_test_scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with support vector machine classification and use probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature  LABEL_BaseExcess\n",
      "Index([           ('Temp', 'mean'),             ('Temp', 'std'),\n",
      "            ('Temp', '<lambda_0>'),           ('RRate', 'mean'),\n",
      "                  ('RRate', 'std'),     ('RRate', '<lambda_0>'),\n",
      "                  ('ABPm', 'mean'),      ('ABPm', '<lambda_0>'),\n",
      "                  ('ABPd', 'amax'),            ('ABPd', 'mean'),\n",
      "            ('ABPd', '<lambda_0>'),            ('SpO2', 'mean'),\n",
      "                   ('SpO2', 'std'),      ('SpO2', '<lambda_0>'),\n",
      "             ('Heartrate', 'mean'), ('Heartrate', '<lambda_0>'),\n",
      "                  ('ABPs', 'mean'),      ('ABPs', '<lambda_0>'),\n",
      "                   ('Hgb', 'mean'),           ('PaCO2', 'amin'),\n",
      "                 ('PaCO2', 'amax'),           ('PaCO2', 'mean'),\n",
      "                  ('FiO2', 'amin'),            ('FiO2', 'amax'),\n",
      "                  ('FiO2', 'mean'),             ('Hct', 'amin'),\n",
      "                   ('Hct', 'amax'),             ('Hct', 'mean'),\n",
      "                    ('pH', 'amin'),              ('pH', 'amax'),\n",
      "                    ('pH', 'mean'),                       'PTT',\n",
      "                         'Lactate',                      'HCO3',\n",
      "                      'BaseExcess',                'Creatinine',\n",
      "                            'SaO2',                   'Calcium',\n",
      "                        'Chloride',                 'TroponinI'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-c54d40a8843b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalues_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvalues_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pid'"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=40)\n",
    "train_features = pd.DataFrame(transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i]))\n",
    "print(\"For feature \", df_train_labels.columns[i])\n",
    "print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "test_features = pd.DataFrame(transformer.transform(data_test_scaled))\n",
    "\n",
    "values_1 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 1]\n",
    "values_0 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 0]\n",
    "values_0 = resample(values_0, replace = False, n_samples = values_1.shape[0])\n",
    "\n",
    "train_features = pd.concat([values_0, values_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(values_0)\n",
    "# print(values_1)\n",
    "# print(values_0.shape)\n",
    "# print(values_1.shape)\n",
    "# print(train_features)\n",
    "\n",
    "    \n",
    "print(np.repeat([0,1], values_0.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "for i in range(1, 12):\n",
    "    \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=40)\n",
    "    train_features = pd.DataFrame(transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i]))\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "    print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = pd.DataFrame(transformer.transform(data_test_scaled))\n",
    "\n",
    "#     values_1 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 1]\n",
    "#     values_0 = train_features.loc[df_train_labels[df_train_labels.columns[i]] == 0]\n",
    "#     values_0 = resample(values_0, replace = False, n_samples = values_1.shape[0])\n",
    "\n",
    "#     train_features = pd.concat([values_0, values_1])\n",
    "    \n",
    "#     labels = np.repeat([0,1], values_0.shape[0])\n",
    "    \n",
    "    #clf = BaggingClassifier(SVC(kernel = 'poly', degree = 5, class_weight = 'balanced', verbose = True, C = 10))\n",
    "    clf_w = SVC(kernel = 'rbf', class_weight = 'balanced', verbose = 2)\n",
    "    \n",
    "    parameters = {'C':(0.1, 1, 5, 10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 5,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 2,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "#     clf.fit(train_features, labels)\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_features)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_features)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "for i in range(12, 16):\n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 50)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "    clf_w = SVR(kernel = 'rbf', gamma = 'scale', cache_size = 6000)\n",
    "# #     clf_w = NuSVR(nu=0.5, kernel = 'linear')\n",
    "    parameters = {'C':(0.1, 1,10)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 5,\n",
    "                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "    print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict with Support vector regression and then compute sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "\n",
    "# columns_1 = [test_pids]\n",
    "\n",
    "# for i in range(1,12):\n",
    "    \n",
    "#     clf = SVR(kernel = 'poly', degree = 3, max_iter = 10000)\n",
    "#     clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     pred = clf.predict(data_test_scaled)\n",
    "#     prob = np.empty(len(pred))\n",
    "#     for j in range(0, len(pred)):\n",
    "#         prob[j] = 1 / (1 + math.exp(-pred[j]))\n",
    "#     columns_1.append(prob)\n",
    "    \n",
    "#     pred_train = clf.predict(data_train_scaled)\n",
    "#     prob_train = np.empty(len(pred_train))\n",
    "#     for j in range(0, len(pred_train)):\n",
    "#         prob_train[j] = 1 / (1 + math.exp(-pred_train[j]))    \n",
    "#     tmp = roc_auc_score(y_score= prob_train, y_true= df_train_labels.iloc[:,i])\n",
    "#     print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #labels that have a real value\n",
    "\n",
    "# columns_2 = []\n",
    "\n",
    "# for i in range(12, 16):\n",
    "    \n",
    "#     # feature selection\n",
    "#     transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "#     train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "#     print(list(data_train_scaled)[transformer.get_support()])\n",
    "#     test_features = transformer.transform(data_test_scaled)\n",
    "    \n",
    "\n",
    "#     clf_w = LinearSVR()\n",
    "#     parameters = {'C':(0.1,1,10,30,60,100)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 2,\n",
    "#                                        refit = True, scoring = 'r2', verbose = 1, n_jobs=6)\n",
    "#     clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "#     pred = clf.predict(test_features)\n",
    "#     columns_2.append(pred)\n",
    "    \n",
    "#     pred_train = clf.predict(train_features)\n",
    "#     tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "#     print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param=20)\n",
    "train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,11])\n",
    "test_features = transformer.transform(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_agg_features.columns[transformer.get_support(indices = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Classifier\n",
    "columns_1 = [test_pids]\n",
    "for i in range(1, 12):\n",
    "    clf = RandomForestClassifier(min_samples_leaf=2, class_weight='balanced', oob_score=False, bootstrap=False)\n",
    "    clf.fit(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(clf.oob_score)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    probability = clf.apply(data_test_scaled)\n",
    "    probs = [i[1] for i in probability] \n",
    "    columns_1.append(probs)\n",
    "    \n",
    "    \n",
    "    probability = clf.predict_proba(data_train_scaled)\n",
    "\n",
    "    probs = [i[1] for i in probability]            \n",
    "    tmp = roc_auc_score(y_score= probs, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the kernel and use SGD Classifier and Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For feature  LABEL_BaseExcess\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.7692051103610277\n",
      "ROC AUC for feature LABEL_BaseExcess  :  0.7726743298225831\n",
      "For feature  LABEL_Fibrinogen\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    3.1s remaining:    1.2s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    9.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.707961517252645\n",
      "ROC AUC for feature LABEL_Fibrinogen  :  0.7192926561929119\n",
      "For feature  LABEL_AST\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    1.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.6693434608965193\n",
      "ROC AUC for feature LABEL_AST  :  0.6742440588675493\n",
      "For feature  LABEL_Alkalinephos\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.6730674571262701\n",
      "ROC AUC for feature LABEL_Alkalinephos  :  0.6786453946637129\n",
      "For feature  LABEL_Bilirubin_total\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.6710610503604066\n",
      "ROC AUC for feature LABEL_Bilirubin_total  :  0.6758456059979748\n",
      "For feature  LABEL_Lactate\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    1.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.7042422493797398\n",
      "ROC AUC for feature LABEL_Lactate  :  0.7088582646637026\n",
      "For feature  LABEL_TroponinI\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    1.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.7028333520611143\n",
      "ROC AUC for feature LABEL_TroponinI  :  0.7122935703374531\n",
      "For feature  LABEL_SaO2\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.7350746791107732\n",
      "ROC AUC for feature LABEL_SaO2  :  0.7387356911226971\n",
      "For feature  LABEL_Bilirubin_direct\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    3.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:   12.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.6871079796595855\n",
      "ROC AUC for feature LABEL_Bilirubin_direct  :  0.7121238506135195\n",
      "For feature  LABEL_EtCO2\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    0.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.79298038633303\n",
      "ROC AUC for feature LABEL_EtCO2  :  0.7995151662585706\n",
      "For feature  LABEL_Sepsis\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 out of  40 | elapsed:    1.0s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.6316632039027443\n",
      "ROC AUC for feature LABEL_Sepsis  :  0.6585471011993339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "# first for the labels that have an output [0,1]\n",
    "test_pids = list(set(df_test_features.pid))\n",
    "columns_1 = [test_pids]\n",
    "\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# train_data = \n",
    "\n",
    "for i in range(1, 12):\n",
    "   \n",
    "    # feature selection\n",
    "    transformer =  GenericUnivariateSelect(score_func= mutual_info_classif, mode ='k_best', param=70)\n",
    "    train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "    print(\"For feature \", df_train_labels.columns[i])\n",
    "#     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "    test_features = transformer.transform(data_test_scaled)\n",
    "\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDClassifier(max_iter=100000, tol=1e-4, penalty = \"l2\", \n",
    "                                       loss = \"epsilon_insensitive\", class_weight='balanced')\n",
    "    # checked before\n",
    "    #parameters = {'alpha':(0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 30)}\n",
    "    parameters = {'alpha':(0.1, 1, 5, 10)}\n",
    "    \n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                        refit = True, scoring = 'roc_auc', verbose = 1,\n",
    "                                       n_jobs=6, return_train_score = True)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "    # compute probabilites as opposed to predictions\n",
    "    #dual_coefficients = clf.dual_coef_    # do we have to normalize with norm of this vector ?\n",
    "    \n",
    "    distance_hyperplane = clf.decision_function(test_features)\n",
    "    probability = np.empty(len(distance_hyperplane))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplane[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplane[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplane[j]))\n",
    "    columns_1.append(probability)\n",
    "\n",
    "    \n",
    "    distance_hyperplace_train = clf.decision_function(train_features)\n",
    "    probability = np.empty(len(distance_hyperplace_train))\n",
    "    for j in range(0, len(probability)):\n",
    "        if distance_hyperplace_train[j] < 0:\n",
    "            probability[j] = 1 - 1/(1 + math.exp(distance_hyperplace_train[j]))\n",
    "        else:\n",
    "            probability[j] = 1/(1 + math.exp(-distance_hyperplace_train[j]))\n",
    "    \n",
    "    tmp = roc_auc_score(y_score= probability, y_true= df_train_labels.iloc[:,i])\n",
    "    print(\"ROC AUC for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  64 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=6)]: Done  79 out of  90 | elapsed:    1.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n",
      "0.38525838303356136\n",
      "R2 for feature LABEL_RRate  :  0.3932819813363555\n",
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n",
      "0.5914401166053944\n",
      "R2 for feature LABEL_ABPm  :  0.6013605375070599\n",
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n",
      "0.36153348125006585\n",
      "R2 for feature LABEL_SpO2  :  0.3767696134465238\n",
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    2.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01}\n",
      "0.6173782261689954\n",
      "R2 for feature LABEL_Heartrate  :  0.6195705679916895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "# labels that have a real value\n",
    "columns_2 = []\n",
    "\n",
    "for i in range(12, 16):\n",
    "    # feature selection\n",
    "#     transformer =  GenericUnivariateSelect(score_func= mutual_info_regression, mode ='k_best', param = 80)\n",
    "#     train_features = transformer.fit_transform(data_train_scaled, df_train_labels.iloc[:,i])\n",
    "# #     print(df_train_agg_features.columns[transformer.get_support(indices = True)])\n",
    "#     test_features = transformer.transform(data_test_scaled)\n",
    "\n",
    "    train_features = data_train_scaled\n",
    "    test_features = data_test_scaled\n",
    "    \n",
    "    feature_map_nystroem = Nystroem(kernel = 'rbf',\n",
    "                                 random_state=1,\n",
    "                                 n_components=300)\n",
    "    train_transformed = feature_map_nystroem.fit_transform(train_features)\n",
    "    test_transformed = feature_map_nystroem.transform(test_features)\n",
    "    \n",
    "    clf_w = linear_model.SGDRegressor(max_iter=100000, tol=1e-4,\n",
    "                                     loss = 'epsilon_insensitive', penalty = 'l2')\n",
    "    parameters = {'alpha':(0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20, 30)}\n",
    "    clf = model_selection.GridSearchCV(estimator= clf_w, param_grid = parameters, cv = 10,\n",
    "                                       refit = True, scoring = 'r2', verbose = 1, n_jobs=6)\n",
    "#     clf = KernelRidge(kernel = 'poly', degree = 5)\n",
    "#     parameters = {'alpha':(0.1,1,10,30)}\n",
    "#     clf = model_selection.GridSearchCV(estimator= clf, param_grid = parameters, cv = 3,\n",
    "#                                       refit = True, scoring = 'r2', verbose = 2, n_jobs=6)\n",
    "    clf.fit(train_features, df_train_labels.iloc[:,i])\n",
    "    \n",
    "#     print(clf.cv_results_)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.best_score_)\n",
    "\n",
    "    pred_train = clf.predict(train_features)\n",
    "    tmp = r2_score(y_pred= pred_train, y_true=df_train_labels.iloc[:,i])\n",
    "    print(\"R2 for feature\", list(df_train_labels)[i] , \" : \", tmp)\n",
    "    \n",
    "    pred = clf.predict(test_features)\n",
    "    columns_2.append(pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_final = columns_1 + columns_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of XGBoost is that it accepts missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 12664)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(columns_final))\n",
    "result = pd.DataFrame(columns_final).transpose()\n",
    "result.columns = list(df_train_labels)\n",
    "result.to_csv('./Results/prediction.csv.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Results/prediction.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
